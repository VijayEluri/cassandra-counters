Index: conf/cassandra.yaml
===================================================================
--- conf/cassandra.yaml	(revision 996701)
+++ conf/cassandra.yaml	(working copy)
@@ -329,6 +329,16 @@
           compare_with: LongType
           comment: 'A column family with supercolumns, whose column names are Longs (8 bytes)'
 
+        - name: IncrementCounter1
+          column_type: Standard
+          clock_type: IncrementCounter
+          reconciler: IncrementCounterReconciler
+
+        - name: SuperIncrementCounter1
+          column_type: Super
+          clock_type: IncrementCounter
+          reconciler: IncrementCounterReconciler
+
         - name: Indexed1
           default_validation_class: LongType
           column_metadata:
Index: interface/cassandra.thrift
===================================================================
--- interface/cassandra.thrift	(revision 996701)
+++ interface/cassandra.thrift	(working copy)
@@ -46,7 +46,7 @@
 #           for every edit that doesn't result in a change to major/minor.
 #
 # See the Semantic Versioning Specification (SemVer) http://semver.org.
-const string VERSION = "15.0.0"
+const string VERSION = "15.1.0"
 
 
 #
@@ -60,7 +60,7 @@
  *                   are made about what the timestamp represents, but using microseconds-since-epoch is customary.
  */
 struct Clock {
-   1: required i64 timestamp,
+   1: optional i64 timestamp,
 }
 
 /** Basic unit of data within a ColumnFamily.
@@ -479,7 +479,17 @@
   */
   void truncate(1:required string cfname)
        throws (1: InvalidRequestException ire, 2: UnavailableException ue),
-    
+  
+  /**
+   Experimental api for increment only counters.
+   Mutation_map maps key to column family to a list of Mutation objects to take place at that scope.
+   Value should be of type long.
+   
+   http://wiki.apache.org/cassandra/Counters
+  */
+  void increment(1:required map<binary, map<string, list<Mutation>>> mutation_map)
+       throws (1:InvalidRequestException ire, 2:UnavailableException ue, 3:TimedOutException te),
+  
   // Meta-APIs -- APIs to get information about the node or cluster,
   // rather than user data.  The nodeprobe program provides usage examples.
   
Index: src/java/org/apache/cassandra/cli/CliClient.java
===================================================================
--- src/java/org/apache/cassandra/cli/CliClient.java	(revision 996701)
+++ src/java/org/apache/cassandra/cli/CliClient.java	(working copy)
@@ -18,6 +18,7 @@
 package org.apache.cassandra.cli;
 
 import org.apache.cassandra.auth.SimpleAuthenticator;
+import org.apache.cassandra.db.ClockType;
 import org.apache.cassandra.db.marshal.AbstractType;
 import org.apache.cassandra.db.marshal.BytesType;
 import org.apache.cassandra.thrift.*;
@@ -499,7 +500,7 @@
             columnName = CliCompiler.getColumn(columnFamilySpec, 1).getBytes("UTF-8");
         }
 
-        Clock thrift_clock = new Clock().setTimestamp(FBUtilities.timestampMicros());
+        Clock thrift_clock = getClock(columnFamily);
         thriftClient_.remove(key.getBytes(), new ColumnPath(columnFamily).setSuper_column(superColumnName).setColumn(columnName),
                              thrift_clock, ConsistencyLevel.ONE);
         css_.out.println(String.format("%s removed.", (columnSpecCnt == 0) ? "row" : "column"));
@@ -683,12 +684,42 @@
         }
         
         // do the insert
-        Clock thrift_clock = new Clock().setTimestamp(FBUtilities.timestampMicros());
+        Clock thrift_clock = getClock(columnFamily);
         thriftClient_.insert(key.getBytes(), new ColumnParent(columnFamily).setSuper_column(superColumnName),
                              new Column(columnName, value.getBytes(), thrift_clock), ConsistencyLevel.ONE);
         
         css_.out.println("Value inserted.");
     }
+
+    private Clock getClock(String columnFamily) throws TException
+    {
+        try
+        {
+            KsDef keyspace = thriftClient_.describe_keyspace(this.keySpace);
+            for (CfDef cf : keyspace.getCf_defs())
+            {
+                if (cf.name.equalsIgnoreCase(columnFamily)) {
+                    ClockType clockType = ClockType.create(cf.clock_type);
+                    switch (clockType)
+                    {
+                    case IncrementCounter:
+                        return new Clock();
+                    default:
+                    case Timestamp:
+                        return new Clock().setTimestamp(FBUtilities.timestampMicros());             
+                    }
+
+                }
+            }
+            
+            // no matching cf found, return default.
+            return new Clock().setTimestamp(FBUtilities.timestampMicros());
+        } catch (NotFoundException e)
+        {
+            // returning default if keyspace is not found
+            return new Clock().setTimestamp(FBUtilities.timestampMicros());
+        }
+    }
     
     private void executeShowClusterName() throws TException
     {
Index: src/java/org/apache/cassandra/config/DatabaseDescriptor.java
===================================================================
--- src/java/org/apache/cassandra/config/DatabaseDescriptor.java	(revision 996701)
+++ src/java/org/apache/cassandra/config/DatabaseDescriptor.java	(working copy)
@@ -805,7 +805,8 @@
         assert tableName != null && cfName != null;
         CFMetaData cfMetaData = getCFMetaData(tableName, cfName);
 
-        assert (cfMetaData != null);
+        if (cfMetaData == null)
+            return null;
         return cfMetaData.clockType;
     }
 
Index: src/java/org/apache/cassandra/db/ClockType.java
===================================================================
--- src/java/org/apache/cassandra/db/ClockType.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/ClockType.java	(working copy)
@@ -21,8 +21,13 @@
 
 public enum ClockType
 {
-    Timestamp;
+    Timestamp,
+    IncrementCounter;
 
+    /**
+     * @param name must match the name of one of the enum values.
+     * @return the clock type specified or null of not found.
+     */
     public final static ClockType create(String name)
     {
         assert name != null;
@@ -38,11 +43,27 @@
 
     public final IClock minClock()
     {
-        return TimestampClock.MIN_VALUE;
+        switch (this)
+        {
+        case Timestamp:
+            return TimestampClock.MIN_VALUE;
+        case IncrementCounter:
+            return IncrementCounterClock.MIN_VALUE;
+        default:
+            return null;
+        }
     }
 
     public final ICompactSerializer2<IClock> serializer()
     {
-        return TimestampClock.SERIALIZER;
+        switch (this)
+        {
+        case Timestamp:
+            return TimestampClock.SERIALIZER;
+        case IncrementCounter:
+            return IncrementCounterClock.SERIALIZER;
+        default:
+            return null;
+        }
     }
 }
Index: src/java/org/apache/cassandra/db/Column.java
===================================================================
--- src/java/org/apache/cassandra/db/Column.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/Column.java	(working copy)
@@ -142,11 +142,7 @@
 
     public IColumn diff(IColumn column)
     {
-        if (ClockRelationship.GREATER_THAN == column.clock().compare(clock))
-        {
-            return column;
-        }
-        return null;
+        return clock.diff(column, this);
     }
 
     public void updateDigest(MessageDigest digest)
Index: src/java/org/apache/cassandra/db/ColumnFamily.java
===================================================================
--- src/java/org/apache/cassandra/db/ColumnFamily.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/ColumnFamily.java	(working copy)
@@ -18,6 +18,7 @@
 
 package org.apache.cassandra.db;
 
+import java.net.InetAddress;
 import java.util.*;
 import java.util.concurrent.ConcurrentSkipListMap;
 import java.security.MessageDigest;
@@ -252,6 +253,17 @@
         }
     }
 
+    /**
+     * Clean the context for the specified node in a clock
+     * specific manner. Used for example in the increment counter
+     * to clear counts from a node.
+     * @param node Node to clear the context from.
+     */
+    public void cleanContext(InetAddress node)
+    {
+        markedForDeleteAt.get().cleanContext(this, node);
+    }
+
     public IColumn getColumn(byte[] name)
     {
         return columns.get(name);
Index: src/java/org/apache/cassandra/db/IClock.java
===================================================================
--- src/java/org/apache/cassandra/db/IClock.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/IClock.java	(working copy)
@@ -20,6 +20,7 @@
 
 import java.io.DataOutput;
 import java.io.IOException;
+import java.net.InetAddress;
 import java.util.List;
 
 /**
@@ -49,6 +50,26 @@
     public IClock getSuperset(List<IClock> otherClocks);
 
     /**
+     * @return compare the two cols and return the left one
+     * if it's greater, otherwise null.
+     */
+    public IColumn diff(IColumn left, IColumn right);
+
+     /**
+      * Clean the context for a specific node.
+      * @param cc Clean the context for the columns in this container.
+      * @param node Node to clean for.
+      */
+    public void cleanContext(IColumnContainer cc, InetAddress node);
+
+    /**
+     * Update context of columns in column family.
+     * @param cf Column family to update.
+     * @param node Update for this node.
+     */
+    public void update(ColumnFamily cf, InetAddress node);
+    
+    /**
      * @return number of bytes this type of clock
      * uses up when serialized.
      */
Index: src/java/org/apache/cassandra/db/IColumnContainer.java
===================================================================
--- src/java/org/apache/cassandra/db/IColumnContainer.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/IColumnContainer.java	(working copy)
@@ -21,6 +21,7 @@
  */
 
 
+import java.util.Collection;
 import org.apache.cassandra.db.marshal.AbstractType;
 
 public interface IColumnContainer
@@ -31,4 +32,14 @@
     public IClock getMarkedForDeleteAt();
 
     public AbstractType getComparator();
+
+    /**
+     * @param name remove column with this name.
+     */
+    public void remove(byte[] name);
+
+    /**
+     * @return the columns in this container sorted by the the specified comparator.
+     */
+    public Collection<IColumn> getSortedColumns();
 }
Index: src/java/org/apache/cassandra/db/IncrementCounterClock.java
===================================================================
--- src/java/org/apache/cassandra/db/IncrementCounterClock.java	(revision 0)
+++ src/java/org/apache/cassandra/db/IncrementCounterClock.java	(revision 0)
@@ -0,0 +1,195 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.apache.cassandra.db;
+
+import java.io.DataInput;
+import java.io.DataOutput;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.apache.cassandra.db.clock.IncrementCounterContext;
+import org.apache.cassandra.io.ICompactSerializer2;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class IncrementCounterClock implements IClock
+{
+    public static final ICompactSerializer2<IClock> SERIALIZER = new IncrementCounterClockSerializer();
+
+    private static IncrementCounterContext contextManager = IncrementCounterContext.instance();
+    
+    public static final IncrementCounterClock MIN_VALUE = new IncrementCounterClock(contextManager.createMin());
+
+    public byte[] context;
+
+    public IncrementCounterClock()
+    {
+        this.context = contextManager.create();
+    }
+
+    public IncrementCounterClock(byte[] context)
+    {
+        this.context = context;
+    }
+
+    public byte[] context()
+    {
+        return context;
+    }
+
+    public void update(InetAddress node, long delta)
+    {
+        context = contextManager.update(context, node, delta);
+    }
+
+    public ClockRelationship compare(IClock other)
+    {
+        assert other instanceof IncrementCounterClock : "Wrong class type.";
+
+        return contextManager.compare(context, ((IncrementCounterClock)other).context());
+    }
+
+    public ClockRelationship diff(IClock other)
+    {
+        assert other instanceof IncrementCounterClock : "Wrong class type.";
+
+        return contextManager.diff(context, ((IncrementCounterClock)other).context());
+    }
+       
+    public IColumn diff(IColumn left, IColumn right)
+    {
+        // data encapsulated in clock
+        if (ClockRelationship.GREATER_THAN == ((IncrementCounterClock)left.clock()).diff(right.clock()))
+        {
+            return left;
+        }
+        return null;
+    }
+
+    public IClock getSuperset(List<IClock> otherClocks)
+    {
+        List<byte[]> contexts = new LinkedList<byte[]>();
+
+        contexts.add(context);
+        for (IClock clock : otherClocks)
+        {
+            assert clock instanceof IncrementCounterClock : "Wrong class type.";
+            contexts.add(((IncrementCounterClock)clock).context);
+        }
+
+        return new IncrementCounterClock(contextManager.merge(contexts));
+    }
+
+    public int size()
+    {
+        return DBConstants.intSize_ + context.length;
+    }
+
+    public void serialize(DataOutput out) throws IOException
+    {
+        SERIALIZER.serialize(this, out);
+    }
+
+    public String toString()
+    {
+        return contextManager.toString(context);
+    }
+
+    protected void cleanNodeCounts(InetAddress node)
+    {
+        context = contextManager.cleanNodeCounts(context, node);
+    }
+
+    public ClockType type()
+    {
+        return ClockType.IncrementCounter;
+    }
+
+    public void cleanContext(IColumnContainer cc, InetAddress node)
+    {
+        for (IColumn column : cc.getSortedColumns())
+        {
+            if (column instanceof SuperColumn)
+            {
+                cleanContext((IColumnContainer)column, node);
+                continue;
+            }
+            IncrementCounterClock clock = (IncrementCounterClock)column.clock();
+            clock.cleanNodeCounts(node);
+            if (0 == clock.context().length)
+                cc.remove(column.name());
+        }
+    }
+
+    public void update(ColumnFamily cf, InetAddress node)
+    {
+        // standard column family
+        if (!cf.isSuper())
+        {
+            for (IColumn col : cf.getSortedColumns())
+            {
+                if (col.isMarkedForDelete())
+                    continue;
+
+                // update in-place, although Column is (abstractly) immutable
+                IncrementCounterClock clock = (IncrementCounterClock)col.clock();
+                clock.update(node, FBUtilities.byteArrayToLong(col.value()));
+            }
+            return;
+        }
+
+        // super column family
+        for (IColumn col : cf.getSortedColumns())
+        {
+            for (IColumn subCol : col.getSubColumns())
+            {
+                if (subCol.isMarkedForDelete())
+                    continue;
+
+                // update in-place, although Column is (abstractly) immutable
+                IncrementCounterClock clock = (IncrementCounterClock)subCol.clock();
+                clock.update(node, FBUtilities.byteArrayToLong(subCol.value()));
+            }
+        }
+    }
+}
+
+class IncrementCounterClockSerializer implements ICompactSerializer2<IClock> 
+{
+    public void serialize(IClock c, DataOutput out) throws IOException
+    {
+        FBUtilities.writeByteArray(((IncrementCounterClock)c).context(), out);
+    }
+
+    public IClock deserialize(DataInput in) throws IOException
+    {
+        int length = in.readInt();
+        if ( length < 0 )
+        {
+            throw new IOException("Corrupt (negative) value length encountered");
+        }
+        byte[] context = new byte[length];
+        if ( length > 0 )
+        {
+            in.readFully(context);
+        }
+        return new IncrementCounterClock(context);
+    }
+}
Index: src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- src/java/org/apache/cassandra/db/RowMutation.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/RowMutation.java	(working copy)
@@ -22,6 +22,8 @@
 import java.io.DataInputStream;
 import java.io.DataOutputStream;
 import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
 import java.util.*;
 import java.util.concurrent.ExecutionException;
 
@@ -35,8 +37,10 @@
 import org.apache.cassandra.service.*;
 import org.apache.cassandra.thrift.ColumnOrSuperColumn;
 import org.apache.cassandra.thrift.Deletion;
+import org.apache.cassandra.thrift.InvalidRequestException;
 import org.apache.cassandra.thrift.Mutation;
 import org.apache.cassandra.thrift.Clock;
+import org.apache.cassandra.thrift.ThriftValidation;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.db.filter.QueryPath;
 import org.apache.cassandra.config.CFMetaData;
@@ -253,13 +257,13 @@
                     assert cosc.super_column != null;
                     for (org.apache.cassandra.thrift.Column column : cosc.super_column.columns)
                     {
-                        rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, unthriftifyClock(column.clock), column.ttl);
+                        rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, unthriftifyClock(rm.getTable(), cfName, column.clock), column.ttl);
                     }
                 }
                 else
                 {
                     assert cosc.super_column == null;
-                    rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, unthriftifyClock(cosc.column.clock), cosc.column.ttl);
+                    rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, unthriftifyClock(rm.getTable(), cfName, cosc.column.clock), cosc.column.ttl);
                 }
             }
         }
@@ -305,36 +309,56 @@
         {
             for (org.apache.cassandra.thrift.Column column : cosc.super_column.columns)
             {
-                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, unthriftifyClock(column.clock), column.ttl);
+                rm.add(new QueryPath(cfName, cosc.super_column.name, column.name), column.value, unthriftifyClock(rm.getTable(), cfName, column.clock), column.ttl);
             }
         }
         else
         {
-            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, unthriftifyClock(cosc.column.clock), cosc.column.ttl);
+            rm.add(new QueryPath(cfName, null, cosc.column.name), cosc.column.value, unthriftifyClock(rm.getTable(), cfName, cosc.column.clock), cosc.column.ttl);
         }
     }
 
     private static void deleteColumnOrSuperColumnToRowMutation(RowMutation rm, String cfName, Deletion del)
     {
+        IClock clock = unthriftifyClock(rm.getTable(), cfName, del.clock);
         if (del.predicate != null && del.predicate.column_names != null)
         {
             for(byte[] c : del.predicate.column_names)
             {
                 if (del.super_column == null && DatabaseDescriptor.getColumnFamilyType(rm.table_, cfName) == ColumnFamilyType.Super)
-                    rm.delete(new QueryPath(cfName, c), unthriftifyClock(del.clock));
+                    rm.delete(new QueryPath(cfName, c), clock);
                 else
-                    rm.delete(new QueryPath(cfName, del.super_column, c), unthriftifyClock(del.clock));
+                    rm.delete(new QueryPath(cfName, del.super_column, c), clock);
             }
         }
         else
         {
-            rm.delete(new QueryPath(cfName, del.super_column), unthriftifyClock(del.clock));
+            rm.delete(new QueryPath(cfName, del.super_column), clock);
         }
     }
 
-    private static IClock unthriftifyClock(Clock clock)
+    private static IClock unthriftifyClock(String keyspace, String cfName, Clock clock)
     {
-        return new TimestampClock(clock.getTimestamp());
+        try
+        {
+            return ThriftValidation.validateClock(keyspace, cfName, clock);
+        }
+        catch (InvalidRequestException e)
+        {
+            return new TimestampClock(clock.getTimestamp()); //default
+        }
+    }
+
+    /**
+     * Update the context of all Columns in this RowMutation
+     */
+    public void updateClocks(InetAddress node)
+    {
+        for (Map.Entry<Integer, ColumnFamily> entry : modifications_.entrySet())
+        {
+            ColumnFamily cf = entry.getValue();
+            cf.getMarkedForDeleteAt().update(cf, node);
+        }
     }
 }
 
Index: src/java/org/apache/cassandra/db/SuperColumn.java
===================================================================
--- src/java/org/apache/cassandra/db/SuperColumn.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/SuperColumn.java	(working copy)
@@ -104,6 +104,11 @@
     {
     	return columns_.values();
     }
+    
+    public Collection<IColumn> getSortedColumns()
+    {
+        return getSubColumns();
+    }
 
     public IColumn getSubColumn(byte[] columnName)
     {
Index: src/java/org/apache/cassandra/db/TimestampClock.java
===================================================================
--- src/java/org/apache/cassandra/db/TimestampClock.java	(revision 996701)
+++ src/java/org/apache/cassandra/db/TimestampClock.java	(working copy)
@@ -21,10 +21,10 @@
 import java.io.DataInput;
 import java.io.DataOutput;
 import java.io.IOException;
+import java.net.InetAddress;
 import java.util.List;
 
 import org.apache.log4j.Logger;
-import org.apache.commons.lang.ArrayUtils;
 
 import org.apache.cassandra.io.ICompactSerializer2;
 
@@ -36,7 +36,7 @@
 {
     private static Logger logger_ = Logger.getLogger(TimestampClock.class);
     public static TimestampClock MIN_VALUE = new TimestampClock(Long.MIN_VALUE);
-    public static TimestampClock ZERO_VALUE = new TimestampClock(0);
+    public static TimestampClock ZERO_VALUE = new TimestampClock(0L);
     public static ICompactSerializer2<IClock> SERIALIZER = new TimestampClockSerializer();
 
     private final long timestamp;
@@ -103,6 +103,23 @@
         return Long.toString(timestamp);
     }
 
+    public IColumn diff(IColumn left, IColumn right)
+    {
+        if (ClockRelationship.GREATER_THAN == left.clock().compare(right.clock()))
+        {
+            return left;
+        }
+        return null;
+    }
+
+    public void cleanContext(IColumnContainer cc, InetAddress node)
+    {
+    }
+
+    public void update(ColumnFamily cf, InetAddress node)
+    {
+    }
+
     @Override
     public boolean equals(Object o)
     {
Index: src/java/org/apache/cassandra/db/clock/IContext.java
===================================================================
--- src/java/org/apache/cassandra/db/clock/IContext.java	(revision 0)
+++ src/java/org/apache/cassandra/db/clock/IContext.java	(revision 0)
@@ -0,0 +1,75 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db.clock;
+
+import java.net.InetAddress;
+import java.util.List;
+
+import org.apache.cassandra.db.IClock.ClockRelationship;
+
+/**
+ * An opaque version context.
+ *
+ * Automatically reconciles multiple versions of values, if possible.  Otherwise,
+ * recognizes version conflicts and calculates the minimum set of values
+ * that need to be reconciled.
+ */
+public interface IContext
+{
+    /**
+     * Creates an initial context.
+     *
+     * @return the initial context.
+     */
+    public byte[] create();
+
+    /**
+     * Determine the version relationship between two contexts.
+     *
+     * EQUAL:        Equal set of nodes and every count is equal.
+     * GREATER_THAN: Superset of nodes and every count is equal or greater than its corollary.
+     * LESS_THAN:    Subset of nodes and every count is equal or less than its corollary.
+     * DISJOINT:     Node sets are not equal and/or counts are not all greater or less than.
+     *
+     * Note: Superset/subset requirements are not strict (because vector length is fixed).
+     *
+     * @param left
+     *            version context.
+     * @param right
+     *            version context.
+     * @return the ContextRelationship between the contexts.
+     */
+    public ClockRelationship compare(byte[] left, byte[] right);
+
+    /**
+     * Return a context that pairwise dominates all of the contexts.
+     *
+     * @param contexts
+     *            a list of contexts to be merged
+     */
+    public byte[] merge(List<byte[]> contexts);
+
+    /**
+     * Human-readable String from context.
+     *
+     * @param context
+     *            version context.
+     * @return a human-readable String of the context.
+     */
+    public String toString(byte[] context);
+}
Index: src/java/org/apache/cassandra/db/clock/IncrementCounterContext.java
===================================================================
--- src/java/org/apache/cassandra/db/clock/IncrementCounterContext.java	(revision 0)
+++ src/java/org/apache/cassandra/db/clock/IncrementCounterContext.java	(revision 0)
@@ -0,0 +1,564 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db.clock;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.Comparator;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import org.apache.cassandra.db.DBConstants;
+import org.apache.cassandra.db.IClock.ClockRelationship;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.commons.lang.ArrayUtils;
+
+/**
+ * An implementation of a distributed increment-only counter context.
+ *
+ * The data structure is:
+ *   1) timestamp,
+ *   2) timestamp of the latest known delete, and
+ *   3) a list of (node id, count) pairs.
+ *
+ * On update:
+ *   1) update timestamp to max(timestamp, local time), and
+ *   2) the node updating the value will increment its associated content.
+ *
+ * The aggregated count can then be determined by rolling up all the counts from each
+ * (node id, count) pair.  NOTE: only a given node id may increment its associated
+ * count and care must be taken that (node id, count) pairs are correctly made
+ * consistent.
+ */
+public class IncrementCounterContext implements IContext
+{
+    public static final int TIMESTAMP_LENGTH = DBConstants.longSize_;
+    public static final int HEADER_LENGTH    = TIMESTAMP_LENGTH * 2; //2xlong
+
+    private static final int idLength;
+    private static final FBUtilities.ByteArrayWrapper idWrapper;
+    private static final int countLength = DBConstants.longSize_;
+    private static final int stepLength; // length: id + count
+
+    // lazy-load singleton
+    private static class LazyHolder
+    {
+        private static final IncrementCounterContext incrementCounterContext = new IncrementCounterContext();
+    }
+
+    static
+    {
+        byte[] id  = FBUtilities.getLocalAddress().getAddress();
+        idLength   = id.length;
+        idWrapper  = new FBUtilities.ByteArrayWrapper(id);
+        stepLength = idLength + countLength;
+    }
+
+    public static IncrementCounterContext instance()
+    {
+        return LazyHolder.incrementCounterContext;
+    }
+
+    /**
+     * Creates an initial counter context.
+     *
+     * @return an empty counter context.
+     */
+    public byte[] create()
+    {
+        byte[] context = new byte[HEADER_LENGTH];
+        FBUtilities.copyIntoBytes(context, 0, System.currentTimeMillis());
+        FBUtilities.copyIntoBytes(context, TIMESTAMP_LENGTH, 0L);
+        return context;
+    }
+    
+    public byte[] createMin()
+    {
+        byte[] rv = new byte[HEADER_LENGTH];
+        FBUtilities.copyIntoBytes(rv, 0, Long.MIN_VALUE);
+        FBUtilities.copyIntoBytes(rv, TIMESTAMP_LENGTH, 0L);
+        return rv;
+    }
+
+    // write a tuple (node id, count) at the front
+    protected static void writeElement(byte[] context, byte[] id, long count)
+    {
+        writeElementAtStepOffset(context, 0, id, count);
+    }
+
+    // write a tuple (node id, count) at step offset
+    protected static void writeElementAtStepOffset(byte[] context, int stepOffset, byte[] id, long count)
+    {
+        int offset = HEADER_LENGTH + (stepOffset * stepLength);
+        System.arraycopy(id, 0, context, offset, idLength);
+        FBUtilities.copyIntoBytes(context, offset + idLength, count);
+    }
+
+    public byte[] update(byte[] context, InetAddress node, long delta)
+    {
+        // update timestamp
+        FBUtilities.copyIntoBytes(context, 0, System.currentTimeMillis());
+        
+        // calculate node id
+        byte[] nodeId = node.getAddress();
+
+        // look for this node id
+        for (int offset = HEADER_LENGTH; offset < context.length; offset += stepLength)
+        {
+            if (FBUtilities.compareByteSubArrays(context, offset, nodeId, 0, idLength) != 0)
+                continue;
+
+            // node id found: increment count, shift to front
+            long count = FBUtilities.byteArrayToLong(context, offset + idLength);
+
+            System.arraycopy(
+                context,
+                HEADER_LENGTH,
+                context,
+                HEADER_LENGTH + stepLength,
+                offset - HEADER_LENGTH);
+            writeElement(context, nodeId, count + delta);
+
+            return context;
+        }
+
+        // node id not found: widen context
+        byte[] previous = context;
+        context = new byte[previous.length + stepLength];
+
+        System.arraycopy(previous, 0, context, 0, HEADER_LENGTH);
+        writeElement(context, nodeId, delta);
+        System.arraycopy(
+            previous,
+            HEADER_LENGTH,
+            context,
+            HEADER_LENGTH + stepLength,
+            previous.length - HEADER_LENGTH);
+
+        return context;
+    }
+
+    // swap bytes of step length in context
+    protected static void swapElement(byte[] context, int left, int right)
+    {
+        if (left == right) return;
+
+        byte temp;
+        for (int i = 0; i < stepLength; i++)
+        {
+            temp = context[left+i];
+            context[left+i] = context[right+i];
+            context[right+i] = temp;
+        }
+    }
+
+    // partition bytes of step length in context (for quicksort)
+    protected static int partitionElements(byte[] context, int left, int right, int pivotIndex)
+    {
+        int leftOffset  = HEADER_LENGTH + (left       * stepLength);
+        int rightOffset = HEADER_LENGTH + (right      * stepLength);
+        int pivotOffset = HEADER_LENGTH + (pivotIndex * stepLength);
+
+        byte[] pivotValue = ArrayUtils.subarray(context, pivotOffset, pivotOffset + stepLength);
+        swapElement(context, pivotOffset, rightOffset);
+        int storeOffset = leftOffset;
+        for (int i = leftOffset; i < rightOffset; i += stepLength)
+        {
+            if (FBUtilities.compareByteSubArrays(context, i, pivotValue, 0, stepLength) <= 0)
+            {
+                swapElement(context, i, storeOffset);
+                storeOffset += stepLength;
+            }
+        }
+        swapElement(context, storeOffset, rightOffset);
+        return (storeOffset - HEADER_LENGTH) / stepLength;
+    }
+
+    // quicksort helper
+    protected static void sortElementsByIdHelper(byte[] context, int left, int right)
+    {
+        if (right <= left) return;
+
+        int pivotIndex = (left + right) / 2;
+        int pivotIndexNew = partitionElements(context, left, right, pivotIndex);
+        sortElementsByIdHelper(context, left, pivotIndexNew - 1);
+        sortElementsByIdHelper(context, pivotIndexNew + 1, right);
+    }
+
+    // quicksort context by id
+    protected static byte[] sortElementsById(byte[] context)
+    {
+        assert 0 == ((context.length - HEADER_LENGTH) % stepLength) : "context size is not correct.";
+        sortElementsByIdHelper(
+            context,
+            0,
+            (int)((context.length - HEADER_LENGTH) / stepLength) - 1);
+        return context;
+    }
+
+    /**
+     * Determine the last modified relationship between two contexts.
+     *
+     * Strategy:
+     *  compare highest timestamp between contexts.
+     *
+     * @param left
+     *            counter context.
+     * @param right
+     *            counter context.
+     * @return the ClockRelationship between the contexts.
+     */
+    public ClockRelationship compare(byte[] left, byte[] right)
+    {
+        long maxDeleteTimestamp  = Math.max(
+                FBUtilities.byteArrayToLong(left,  TIMESTAMP_LENGTH), 
+                FBUtilities.byteArrayToLong(right, TIMESTAMP_LENGTH));
+        
+        long leftTimestamp  = FBUtilities.byteArrayToLong(left,  0);
+        long rightTimestamp = FBUtilities.byteArrayToLong(right, 0);
+        
+        // obsolete context due to being older then the last known delete
+        if (leftTimestamp < maxDeleteTimestamp)
+        {
+            return ClockRelationship.LESS_THAN;
+        } 
+        else if (rightTimestamp < maxDeleteTimestamp)
+        {
+            return ClockRelationship.GREATER_THAN;
+        }
+        
+        if (leftTimestamp < rightTimestamp)
+        {
+            return ClockRelationship.LESS_THAN;
+        }
+        else if (leftTimestamp == rightTimestamp)
+        {
+            return ClockRelationship.EQUAL;
+        }
+        return ClockRelationship.GREATER_THAN;
+    }
+
+    /**
+     * Determine the count relationship between two contexts.
+     *
+     * Strategy:
+     *  compare node count values (like a version vector).
+     *
+     * @param left
+     *            counter context.
+     * @param right
+     *            counter context.
+     * @return the ClockRelationship between the contexts.
+     */
+    public ClockRelationship diff(byte[] left, byte[] right)
+    {
+        left = sortElementsById(left);
+        right = sortElementsById(right);
+
+        ClockRelationship relationship = ClockRelationship.EQUAL;
+
+        int leftIndex = HEADER_LENGTH;
+        int rightIndex = HEADER_LENGTH;
+        while (leftIndex < left.length && rightIndex < right.length)
+        {
+            // compare id bytes
+            int compareId = FBUtilities.compareByteSubArrays(left, leftIndex, right, rightIndex, idLength);
+            if (compareId == 0)
+            {
+                long leftCount = FBUtilities.byteArrayToLong(left, leftIndex + idLength);
+                long rightCount = FBUtilities.byteArrayToLong(right, rightIndex + idLength);
+
+                // advance indexes
+                leftIndex += stepLength;
+                rightIndex += stepLength;
+
+                // process count comparisons
+                if (leftCount == rightCount)
+                {
+                    continue;
+                }
+                else if (leftCount > rightCount)
+                {
+                    if (relationship == ClockRelationship.EQUAL)
+                    {
+                        relationship = ClockRelationship.GREATER_THAN;
+                    }
+                    else if (relationship == ClockRelationship.GREATER_THAN)
+                    {
+                        continue;
+                    }
+                    else
+                    {
+                        // relationship == ClockRelationship.LESS_THAN
+                        return ClockRelationship.DISJOINT;
+                    }
+                }
+                else
+                // leftCount < rightCount
+                {
+                    if (relationship == ClockRelationship.EQUAL)
+                    {
+                        relationship = ClockRelationship.LESS_THAN;
+                    }
+                    else if (relationship == ClockRelationship.GREATER_THAN)
+                    {
+                        return ClockRelationship.DISJOINT;
+                    }
+                    else
+                    {
+                        // relationship == ClockRelationship.LESS_THAN
+                        continue;
+                    }
+                }
+            }
+            else if (compareId > 0)
+            {
+                // only advance the right context
+                rightIndex += stepLength;
+
+                if (relationship == ClockRelationship.EQUAL)
+                {
+                    relationship = ClockRelationship.LESS_THAN;
+                }
+                else if (relationship == ClockRelationship.GREATER_THAN)
+                {
+                    return ClockRelationship.DISJOINT;
+                }
+                else
+                {
+                    // relationship == ClockRelationship.LESS_THAN
+                    continue;
+                }
+            }
+            else
+            {
+                // compareId < 0
+                // only advance the left context
+                leftIndex += stepLength;
+
+                if (relationship == ClockRelationship.EQUAL)
+                {
+                    relationship = ClockRelationship.GREATER_THAN;
+                }
+                else if (relationship == ClockRelationship.GREATER_THAN)
+                {
+                    continue;
+                }
+                else
+                // relationship == ClockRelationship.LESS_THAN
+                {
+                    return ClockRelationship.DISJOINT;
+                }
+            }
+        }
+
+        // check final lengths
+        if (leftIndex < left.length)
+        {
+            if (relationship == ClockRelationship.EQUAL)
+            {
+                return ClockRelationship.GREATER_THAN;
+            }
+            else if (relationship == ClockRelationship.LESS_THAN)
+            {
+                return ClockRelationship.DISJOINT;
+            }
+        }
+        else if (rightIndex < right.length)
+        {
+            if (relationship == ClockRelationship.EQUAL)
+            {
+                return ClockRelationship.LESS_THAN;
+            }
+            else if (relationship == ClockRelationship.GREATER_THAN)
+            {
+                return ClockRelationship.DISJOINT;
+            }
+        }
+
+        return relationship;
+    }
+
+    /**
+     * Return a context w/ an aggregated count for each node id.
+     *
+     * @param contexts
+     *            a list of contexts to be merged
+     */
+    public byte[] merge(List<byte[]> contexts)
+    {
+        // strategy:
+        //   1) take highest timestamp
+        //   2) take highest delete timestamp
+        //   3) map id -> count
+        //      a) local id:  sum counts; keep highest timestamp
+        //      b) remote id: keep highest count (reconcile)
+        //   4) create a context from sorted array
+        long highestTimestamp = Long.MIN_VALUE;
+        long highestDeleteTimestamp = Long.MIN_VALUE;
+        Map<FBUtilities.ByteArrayWrapper, Long> contextsMap =
+            new HashMap<FBUtilities.ByteArrayWrapper, Long>();
+        for (byte[] context : contexts)
+        {
+            // take highest timestamp
+            highestTimestamp = Math.max(FBUtilities.byteArrayToLong(context, 0), highestTimestamp);
+            highestDeleteTimestamp = Math.max(FBUtilities.byteArrayToLong(context, TIMESTAMP_LENGTH), highestDeleteTimestamp);
+
+            // map id -> count
+            for (int offset = HEADER_LENGTH; offset < context.length; offset += stepLength)
+            {
+                FBUtilities.ByteArrayWrapper id = new FBUtilities.ByteArrayWrapper(
+                        ArrayUtils.subarray(context, offset, offset + idLength));
+                long count = FBUtilities.byteArrayToLong(context, offset + idLength);
+
+                Long previousCount = contextsMap.put(id, count);
+                if (previousCount == null)
+                    continue;
+
+                // local id: sum counts
+                if (this.idWrapper.equals(id)) {
+                    contextsMap.put(id, count + previousCount);
+                    continue;
+                }
+
+                // remote id: keep highest count
+                contextsMap.put(id, Math.max(count, previousCount));
+            }
+        }
+
+        List<Map.Entry<FBUtilities.ByteArrayWrapper, Long>> contextsList =
+            new ArrayList<Map.Entry<FBUtilities.ByteArrayWrapper, Long>>(
+                    contextsMap.entrySet());
+        Collections.sort(
+            contextsList,
+            new Comparator<Map.Entry<FBUtilities.ByteArrayWrapper, Long>>()
+            {
+                public int compare(
+                    Map.Entry<FBUtilities.ByteArrayWrapper, Long> e1,
+                    Map.Entry<FBUtilities.ByteArrayWrapper, Long> e2)
+                {
+                    // reversed
+                    int result = e2.getValue().compareTo(e1.getValue());
+                    if (result != 0)
+                        return result;
+                    return FBUtilities.compareByteArrays(e2.getKey().data, e1.getKey().data);
+                }
+            });
+
+        int length = contextsList.size();
+        byte[] merged = new byte[HEADER_LENGTH + (length * stepLength)];
+        FBUtilities.copyIntoBytes(merged, 0, highestTimestamp);
+        FBUtilities.copyIntoBytes(merged, TIMESTAMP_LENGTH, highestDeleteTimestamp);
+        for (int i = 0; i < length; i++)
+        {
+            Map.Entry<FBUtilities.ByteArrayWrapper, Long> entry = contextsList.get(i);
+            writeElementAtStepOffset(
+                merged,
+                i,
+                entry.getKey().data,
+                entry.getValue().longValue());
+        }
+        return merged;
+    }
+
+    /**
+     * Human-readable String from context.
+     *
+     * @param context
+     *            version context.
+     * @return a human-readable String of the context.
+     */
+    public String toString(byte[] context)
+    {
+        context = sortElementsById(context);
+
+        StringBuilder sb = new StringBuilder();
+        sb.append("{");
+        sb.append(FBUtilities.byteArrayToLong(context, 0));
+        sb.append(", ");
+        sb.append(FBUtilities.byteArrayToLong(context, TIMESTAMP_LENGTH));
+        sb.append(" + [");
+        for (int offset = HEADER_LENGTH; offset < context.length; offset += stepLength)
+        {
+            if (offset != HEADER_LENGTH)
+            {
+                sb.append(",");
+            }
+            sb.append("(");
+            try
+            {
+                InetAddress address = InetAddress.getByAddress(
+                            ArrayUtils.subarray(context, offset, offset + idLength));
+                sb.append(address.getHostAddress());
+            }
+            catch (UnknownHostException uhe)
+            {
+                sb.append("?.?.?.?");
+            }
+            sb.append(", ");
+            sb.append(FBUtilities.byteArrayToLong(context, offset + idLength));
+            sb.append(")");
+        }
+        sb.append("]}");
+        return sb.toString();
+    }
+
+    // return an aggregated count across all node ids
+    public byte[] total(byte[] context)
+    {
+        long total = 0;
+
+        for (int offset = HEADER_LENGTH; offset < context.length; offset += stepLength)
+        {
+            long count = FBUtilities.byteArrayToLong(context, offset + idLength);
+            total += count;
+        }
+
+        return FBUtilities.toByteArray(total);
+    }
+
+    // remove the count for a given node id
+    public byte[] cleanNodeCounts(byte[] context, InetAddress node)
+    {
+        // calculate node id
+        byte[] nodeId = node.getAddress();
+
+        // look for this node id
+        for (int offset = HEADER_LENGTH; offset < context.length; offset += stepLength)
+        {
+            if (FBUtilities.compareByteSubArrays(context, offset, nodeId, 0, idLength) != 0)
+                continue;
+
+            // node id found: remove node count
+            byte[] truncatedContext = new byte[context.length - stepLength];
+            System.arraycopy(context, 0, truncatedContext, 0, offset);
+            System.arraycopy(
+                context,
+                offset + stepLength,
+                truncatedContext,
+                offset,
+                context.length - (offset + stepLength));
+            return truncatedContext;
+        }
+
+        return context;
+    }
+}
Index: src/java/org/apache/cassandra/db/clock/IncrementCounterReconciler.java
===================================================================
--- src/java/org/apache/cassandra/db/clock/IncrementCounterReconciler.java	(revision 0)
+++ src/java/org/apache/cassandra/db/clock/IncrementCounterReconciler.java	(revision 0)
@@ -0,0 +1,141 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.db.clock;
+
+import java.util.List;
+import java.util.LinkedList;
+
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.DBConstants;
+import org.apache.cassandra.db.DeletedColumn;
+import org.apache.cassandra.db.IClock;
+import org.apache.cassandra.db.IncrementCounterClock;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class IncrementCounterReconciler extends AbstractReconciler
+{
+    private static final IncrementCounterContext contextManager = IncrementCounterContext.instance();
+
+    public static final IncrementCounterReconciler instance = new IncrementCounterReconciler();
+
+    private IncrementCounterReconciler()
+    {/* singleton */}
+
+    private IClock mergeClocks(Column left, Column right)
+    {
+        List<IClock> clocks = new LinkedList<IClock>();
+        clocks.add(right.clock());
+        return (IClock)left.clock().getSuperset(clocks);
+    }
+
+    // note: called in addColumn(IColumn) to aggregate local node id's counts
+    public Column reconcile(Column left, Column right)
+    {
+        IncrementCounterClock leftClock = (IncrementCounterClock) left.clock();
+        IncrementCounterClock rightClock = (IncrementCounterClock) right.clock();
+        long maxDeleteTimestamp = Math.max(FBUtilities.byteArrayToLong(leftClock.context, IncrementCounterContext.TIMESTAMP_LENGTH),
+                FBUtilities.byteArrayToLong(rightClock.context, IncrementCounterContext.TIMESTAMP_LENGTH));
+        long leftTimestamp = FBUtilities.byteArrayToLong(leftClock.context);
+        long rightTimestamp = FBUtilities.byteArrayToLong(rightClock.context);
+        
+        // count as deleted if the timestamp is older then the highest known delete timestamp
+        boolean leftDeleted = left.isMarkedForDelete() || leftTimestamp < maxDeleteTimestamp;
+        boolean rightDeleted = right.isMarkedForDelete() || rightTimestamp < maxDeleteTimestamp;
+        if (leftDeleted)
+        {
+            if (rightDeleted)
+            {
+                // delete + delete: keep later tombstone, higher clock
+                int leftLocalDeleteTime  = FBUtilities.byteArrayToInt(left.value());
+                int rightLocalDeleteTime = FBUtilities.byteArrayToInt(right.value());
+
+                return new DeletedColumn(
+                    left.name(),
+                    leftLocalDeleteTime >= rightLocalDeleteTime ? left.value() : right.value(),
+                    mergeClocks(left, right));
+            }
+
+            updateDeleteTimestamp(left.clock(), right.clock());
+            // delete + live: use compare() to determine which side to take
+            // note: tombstone always wins ties.
+            switch (left.clock().compare(right.clock()))
+            {
+                case EQUAL:
+                case GREATER_THAN:
+                    return left;
+                case LESS_THAN:
+                    return right;
+                default:
+                    throw new IllegalArgumentException("Unexpected situation, clock comparison was DISJOINT: " + left.clock() + " - " + right.clock());
+                    // note: DISJOINT is not possible
+            }
+        }
+        else if (rightDeleted)
+        {
+            updateDeleteTimestamp(right.clock(), left.clock());
+            // live + delete: use compare() to determine which side to take
+            // note: tombstone always wins ties.
+            switch (left.clock().compare(right.clock()))
+            {
+                case GREATER_THAN:
+                    return left;
+                case EQUAL:
+                case LESS_THAN:
+                    return right;
+                default:
+                    throw new IllegalArgumentException("Unexpected situation, clock comparison was DISJOINT: " + left.clock() + " - " + right.clock());
+                    // note: DISJOINT is not possible
+            }
+        }
+        else
+        {            
+            // live + live: merge clocks; update value
+            IClock clock = mergeClocks(left, right);
+            IncrementCounterClock counterClock = (IncrementCounterClock) clock;
+            // only timestamp and delete timestamp in the clock, has not yet had update called on it.
+            // for example multiple mutates on one column in a batch
+            if (counterClock.context.length == IncrementCounterContext.HEADER_LENGTH) 
+            {
+                long total = 0;
+                if (left.value().length == DBConstants.longSize_ && right.value().length == DBConstants.longSize_)
+                {
+                    total = FBUtilities.byteArrayToLong(left.value()) + FBUtilities.byteArrayToLong(right.value());
+                } else if (left.value().length == DBConstants.longSize_)
+                {
+                    total = FBUtilities.byteArrayToLong(left.value());
+                } else if (right.value().length == DBConstants.longSize_)
+                {
+                    total = FBUtilities.byteArrayToLong(right.value());
+                }
+                return new Column(left.name(), FBUtilities.toByteArray(total), clock);
+            }
+            
+            byte[] value = contextManager.total(((IncrementCounterClock)clock).context());
+            return new Column(left.name(), value, clock);
+        }
+    }
+
+    private void updateDeleteTimestamp(IClock deletedClock, IClock liveClock)
+    {
+        IncrementCounterClock dc = (IncrementCounterClock) deletedClock;
+        IncrementCounterClock lc = (IncrementCounterClock) liveClock;
+        long deleteTime = Math.max(FBUtilities.byteArrayToLong(dc.context, 0), 
+                FBUtilities.byteArrayToLong(lc.context, IncrementCounterContext.TIMESTAMP_LENGTH));
+        FBUtilities.copyIntoBytes(lc.context, IncrementCounterContext.TIMESTAMP_LENGTH, deleteTime);
+    }
+}
Index: src/java/org/apache/cassandra/dht/BootStrapper.java
===================================================================
--- src/java/org/apache/cassandra/dht/BootStrapper.java	(revision 996701)
+++ src/java/org/apache/cassandra/dht/BootStrapper.java	(working copy)
@@ -35,6 +35,7 @@
  import org.apache.cassandra.locator.AbstractReplicationStrategy;
  import org.apache.cassandra.net.*;
  import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.streaming.OperationType;
  import org.apache.cassandra.streaming.StreamIn;
  import org.apache.cassandra.utils.SimpleCondition;
  import org.apache.cassandra.utils.FBUtilities;
@@ -79,7 +80,7 @@
                 StorageService.instance.addBootstrapSource(source, table);
                 if (logger.isDebugEnabled())
                     logger.debug("Requesting from " + source + " ranges " + StringUtils.join(entry.getValue(), ", "));
-                StreamIn.requestRanges(source, table, entry.getValue());
+                StreamIn.requestRanges(source, table, entry.getValue(), OperationType.BOOTSTRAP);
             }
         }
     }
Index: src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
===================================================================
--- src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java	(revision 996701)
+++ src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java	(working copy)
@@ -245,7 +245,7 @@
 
     private Clock avroToThrift(org.apache.cassandra.avro.Clock aclo)
     {
-        return new Clock(aclo.timestamp);
+        return new Clock().setTimestamp(aclo.timestamp);
     }
 
     /**
Index: src/java/org/apache/cassandra/io/sstable/AESRecoveryProcessor.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/AESRecoveryProcessor.java	(revision 0)
+++ src/java/org/apache/cassandra/io/sstable/AESRecoveryProcessor.java	(revision 0)
@@ -0,0 +1,131 @@
+/**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.apache.cassandra.io.sstable;
+
+import java.io.IOError;
+import java.io.IOException;
+import java.net.InetAddress;
+
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class AESRecoveryProcessor implements IRecoveryProcessor
+{
+    private static Logger logger = LoggerFactory.getLogger(AESRecoveryProcessor.class);
+
+    private InetAddress addr;
+    
+    /**
+     * @param addr clean the information regarding this addr in the context 
+     */
+    public AESRecoveryProcessor(InetAddress addr) {
+        this.addr = addr;
+    }
+    
+    /**
+     * Removes the given SSTable from temporary status and opens it, rebuilding the SSTable.
+     *
+     * @param desc Descriptor for the SSTable file
+     */
+    public void recover(Descriptor desc) throws IOException
+    {
+        ColumnFamilyStore cfs = Table.open(desc.ksname).getColumnFamilyStore(desc.cfname);
+
+        // open the data file for input, and write out a "cleaned" version
+        try
+        {
+            FBUtilities.renameWithConfirm(
+                desc.filenameFor(SSTable.COMPONENT_DATA),
+                desc.filenameFor(SSTable.COMPONENT_STREAMED));
+        }
+        catch (IOException e)
+        {
+            throw new IOError(e);
+        }
+        BufferedRandomAccessFile dfile = new BufferedRandomAccessFile(
+            desc.filenameFor(SSTable.COMPONENT_STREAMED),
+            "r",
+            8 * 1024 * 1024);
+
+        // rebuild sstable          
+        SSTableWriter writer;
+        try
+        {            
+            long estimatedRows = SSTableWriter.estimateRows(desc, dfile);            
+            long expectedBloomFilterSize = Math.max(
+                SSTableReader.indexInterval(),
+                estimatedRows);
+            writer = new SSTableWriter(
+                desc.filenameFor(SSTable.COMPONENT_DATA),
+                expectedBloomFilterSize,
+                cfs.metadata,
+                cfs.partitioner);
+        }
+        catch(IOException e)
+        {
+            dfile.close();
+            throw e;
+        }
+
+        try
+        {
+            DecoratedKey key;
+            long dataPosition = 0;
+            while (dataPosition < dfile.length())
+            {
+                key = SSTableReader.decodeKey(
+                    StorageService.getPartitioner(),
+                    desc,
+                    FBUtilities.readShortByteArray(dfile));
+                long dataSize = SSTableReader.readRowSize(dfile, desc);
+                dataPosition = dfile.getFilePointer() + dataSize;
+
+                // skip bloom filter and column index
+                dfile.skipBytes(dfile.readInt());
+                dfile.skipBytes(dfile.readInt());
+
+                // clean the column data
+                ColumnFamily cf = ColumnFamily.create(desc.ksname, desc.cfname);
+                ColumnFamily.serializer().deserializeFromSSTableNoColumns(cf, dfile);
+                ColumnFamily.serializer().deserializeColumns(dfile, cf);
+                cf.cleanContext(addr);
+
+                writer.append(key, cf);
+            }
+        }
+        finally
+        {
+            try
+            {
+                dfile.close();
+            }
+            catch (IOException e)
+            {
+                logger.error("Failed to close data or aes file during recovery of " + desc, e);
+            }
+        }
+
+        writer.closeAndOpenReader();
+    }
+}
Index: src/java/org/apache/cassandra/io/sstable/Component.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/Component.java	(revision 996701)
+++ src/java/org/apache/cassandra/io/sstable/Component.java	(working copy)
@@ -50,8 +50,10 @@
         // statistical metadata about the content of the sstable
         STATS("Statistics.db"),
         // a bitmap secondary index: many of these may exist per sstable
-        BITMAP_INDEX("Bitidx.db");
-
+        BITMAP_INDEX("Bitidx.db"),
+        // a newly streamed file, pre recovery
+        STREAMED("Streamed");
+        
         final String repr;
         Type(String repr)
         {
@@ -73,6 +75,7 @@
     public final static Component FILTER = new Component(Type.FILTER, -1);
     public final static Component COMPACTED_MARKER = new Component(Type.COMPACTED_MARKER, -1);
     public final static Component STATS = new Component(Type.STATS, -1);
+    public final static Component STREAMED = new Component(Type.STREAMED, -1);
 
     public final Type type;
     public final int id;
@@ -101,6 +104,7 @@
             case PRIMARY_INDEX:
             case FILTER:
             case COMPACTED_MARKER:
+            case STREAMED:
             case STATS:
                 return type.repr;
             case BITMAP_INDEX:
@@ -138,6 +142,7 @@
             case FILTER:            component = Component.FILTER;           break;
             case COMPACTED_MARKER:  component = Component.COMPACTED_MARKER; break;
             case STATS:             component = Component.STATS;            break;
+            case STREAMED:          component = Component.STREAMED;         break;
             case BITMAP_INDEX:
                  component = new Component(type, id);
                  break;
Index: src/java/org/apache/cassandra/io/sstable/IRecoveryProcessor.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/IRecoveryProcessor.java	(revision 0)
+++ src/java/org/apache/cassandra/io/sstable/IRecoveryProcessor.java	(revision 0)
@@ -0,0 +1,34 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable;
+
+import java.io.IOException;
+
+/**
+ * Recovers an SSTable.
+ */
+public interface IRecoveryProcessor
+{
+
+    /**
+     * Removes the given SSTable from temporary status and opens it, rebuilding the SSTable as appropriate.
+     *
+     * @param desc Descriptor for the SSTable file
+     */
+    public void recover(Descriptor desc) throws IOException;
+}
Index: src/java/org/apache/cassandra/io/sstable/IndexRecoveryProcessor.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/IndexRecoveryProcessor.java	(revision 0)
+++ src/java/org/apache/cassandra/io/sstable/IndexRecoveryProcessor.java	(revision 0)
@@ -0,0 +1,156 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.io.sstable;
+
+import java.io.IOException;
+import java.util.HashSet;
+import java.util.Set;
+import java.util.concurrent.ExecutionException;
+
+import org.apache.commons.lang.ArrayUtils;
+import org.slf4j.Logger;
+import org.slf4j.LoggerFactory;
+
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.io.util.BufferedRandomAccessFile;
+import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * As a recovery is run over an SSTable the columns may require modification.
+ * This interface provides us with a method of doing so.
+ */
+public class IndexRecoveryProcessor implements IRecoveryProcessor
+{
+    private static Logger logger = LoggerFactory.getLogger(IndexRecoveryProcessor.class);
+
+    // lazy-load singleton
+    private static class LazyHolder
+    {
+        private static final IndexRecoveryProcessor indexRecoveryProcessor = new IndexRecoveryProcessor();
+    }
+
+    public static IndexRecoveryProcessor instance()
+    {
+        return LazyHolder.indexRecoveryProcessor;
+    }
+
+    /**
+     * Removes the given SSTable from temporary status and opens it, rebuilding the non-essential portions of the
+     * file if necessary.
+     * TODO: Builds most of the in-memory state of the sstable, but doesn't actually open it.
+     *
+     * @param desc Descriptor for the SSTable file
+     */
+    public void recover(Descriptor desc) throws IOException
+    {
+        ColumnFamilyStore cfs = Table.open(desc.ksname).getColumnFamilyStore(desc.cfname);
+        Set<byte[]> indexedColumns = cfs.getIndexedColumns();
+
+        // open the data file for input, and an IndexWriter for output
+        BufferedRandomAccessFile dfile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_DATA), "r", 8 * 1024 * 1024);
+        SSTableWriter.IndexWriter iwriter;
+        long estimatedRows;
+        try
+        {            
+            estimatedRows = SSTableWriter.estimateRows(desc, dfile);
+            iwriter = new SSTableWriter.IndexWriter(desc, StorageService.getPartitioner(), estimatedRows);
+        }
+        catch(IOException e)
+        {
+            dfile.close();
+            throw e;
+        }
+
+        // build the index and filter
+        long rows = 0;
+        try
+        {
+            DecoratedKey key;
+            long dataPosition = 0;
+            while (dataPosition < dfile.length())
+            {
+                key = SSTableReader.decodeKey(StorageService.getPartitioner(), desc, FBUtilities.readShortByteArray(dfile));
+                long dataSize = SSTableReader.readRowSize(dfile, desc);
+                if (!indexedColumns.isEmpty())
+                {
+                    // skip bloom filter and column index
+                    dfile.readFully(new byte[dfile.readInt()]);
+                    dfile.readFully(new byte[dfile.readInt()]);
+
+                    // index the column data
+                    ColumnFamily cf = ColumnFamily.create(desc.ksname, desc.cfname);
+                    ColumnFamily.serializer().deserializeFromSSTableNoColumns(cf, dfile);
+                    int columns = dfile.readInt();
+                    for (int i = 0; i < columns; i++)
+                    {
+                        IColumn iColumn = cf.getColumnSerializer().deserialize(dfile);
+                        if (indexedColumns.contains(iColumn.name()))
+                        {
+                            DecoratedKey valueKey = cfs.getIndexKeyFor(iColumn.name(), iColumn.value());
+                            ColumnFamily indexedCf = cfs.newIndexedColumnFamily(iColumn.name());
+                            indexedCf.addColumn(new Column(key.key, ArrayUtils.EMPTY_BYTE_ARRAY, iColumn.clock()));
+                            logger.debug("adding indexed column row mutation for key {}", valueKey);
+                            Table.open(desc.ksname).applyIndexedCF(cfs.getIndexedColumnFamilyStore(iColumn.name()),
+                                                                   key,
+                                                                   valueKey,
+                                                                   indexedCf);
+                        }
+                    }
+                }
+
+                iwriter.afterAppend(key, dataPosition);
+                dataPosition = dfile.getFilePointer() + dataSize;
+                dfile.seek(dataPosition);
+                rows++;
+            }
+
+            for (byte[] column : cfs.getIndexedColumns())
+            {
+                try
+                {
+                    cfs.getIndexedColumnFamilyStore(column).forceBlockingFlush();
+                }
+                catch (ExecutionException e)
+                {
+                    throw new RuntimeException(e);
+                }
+                catch (InterruptedException e)
+                {
+                    throw new AssertionError(e);
+                }
+            }
+        }
+        finally
+        {
+            try
+            {
+                dfile.close();
+                iwriter.close();
+            }
+            catch (IOException e)
+            {
+                logger.error("Failed to close data or index file during recovery of " + desc, e);
+            }
+        }
+
+        SSTableWriter.rename(desc, SSTable.componentsFor(desc));
+
+        logger.debug("estimated row count was %s of real count", ((double)estimatedRows) / rows);
+    }
+}
Index: src/java/org/apache/cassandra/io/sstable/SSTable.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/SSTable.java	(revision 996701)
+++ src/java/org/apache/cassandra/io/sstable/SSTable.java	(working copy)
@@ -57,6 +57,7 @@
     public static final String COMPONENT_STATS = Component.Type.STATS.repr;
 
     public static final String COMPONENT_COMPACTED = Component.Type.COMPACTED_MARKER.repr;
+    public static final String COMPONENT_STREAMED = Component.Type.STREAMED.repr;
 
     protected final Descriptor desc;
     protected final Set<Component> components;
Index: src/java/org/apache/cassandra/io/sstable/SSTableWriter.java
===================================================================
--- src/java/org/apache/cassandra/io/sstable/SSTableWriter.java	(revision 996701)
+++ src/java/org/apache/cassandra/io/sstable/SSTableWriter.java	(working copy)
@@ -21,9 +21,7 @@
 
 import java.io.*;
 import java.util.Set;
-import java.util.concurrent.ExecutionException;
 
-import org.apache.commons.lang.ArrayUtils;
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
@@ -35,6 +33,7 @@
 import org.apache.cassandra.io.util.BufferedRandomAccessFile;
 import org.apache.cassandra.io.util.SegmentedFile;
 import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.io.sstable.IRecoveryProcessor;
 import org.apache.cassandra.utils.BloomFilter;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.EstimatedHistogram;
@@ -193,7 +192,7 @@
     /**
      * @return An estimate of the number of keys contained in the given data file.
      */
-    private static long estimateRows(Descriptor desc, BufferedRandomAccessFile dfile) throws IOException
+    protected static long estimateRows(Descriptor desc, BufferedRandomAccessFile dfile) throws IOException
     {
         // collect sizes for the first 1000 keys, or first 100 megabytes of data
         final int SAMPLES_CAP = 1000, BYTES_CAP = (int)Math.min(100000000, dfile.length());
@@ -212,136 +211,42 @@
     }
 
     /**
-     * If either of the index or filter files are missing, rebuilds both.
-     * TODO: Builds most of the in-memory state of the sstable, but doesn't actually open it.
-     */
-    private static void maybeRecover(Descriptor desc) throws IOException
-    {
-        logger.debug("In maybeRecover with Descriptor {}", desc);
-        File ifile = new File(desc.filenameFor(SSTable.COMPONENT_INDEX));
-        File ffile = new File(desc.filenameFor(SSTable.COMPONENT_FILTER));
-        if (ifile.exists() && ffile.exists())
-            // nothing to do
-            return;
-
-        ColumnFamilyStore cfs = Table.open(desc.ksname).getColumnFamilyStore(desc.cfname);
-        Set<byte[]> indexedColumns = cfs.getIndexedColumns();
-        // remove existing files
-        ifile.delete();
-        ffile.delete();
-
-        // open the data file for input, and an IndexWriter for output
-        BufferedRandomAccessFile dfile = new BufferedRandomAccessFile(desc.filenameFor(SSTable.COMPONENT_DATA), "r", 8 * 1024 * 1024);
-        IndexWriter iwriter;
-        long estimatedRows;
-        try
-        {            
-            estimatedRows = estimateRows(desc, dfile);            
-            iwriter = new IndexWriter(desc, StorageService.getPartitioner(), estimatedRows);
-        }
-        catch(IOException e)
-        {
-            dfile.close();
-            throw e;
-        }
-
-        // build the index and filter
-        long rows = 0;
-        try
-        {
-            DecoratedKey key;
-            long dataPosition = 0;
-            while (dataPosition < dfile.length())
-            {
-                key = SSTableReader.decodeKey(StorageService.getPartitioner(), desc, FBUtilities.readShortByteArray(dfile));
-                long dataSize = SSTableReader.readRowSize(dfile, desc);
-                if (!indexedColumns.isEmpty())
-                {
-                    // skip bloom filter and column index
-                    dfile.readFully(new byte[dfile.readInt()]);
-                    dfile.readFully(new byte[dfile.readInt()]);
-
-                    // index the column data
-                    ColumnFamily cf = ColumnFamily.create(desc.ksname, desc.cfname);
-                    ColumnFamily.serializer().deserializeFromSSTableNoColumns(cf, dfile);
-                    int columns = dfile.readInt();
-                    for (int i = 0; i < columns; i++)
-                    {
-                        IColumn iColumn = cf.getColumnSerializer().deserialize(dfile);
-                        if (indexedColumns.contains(iColumn.name()))
-                        {
-                            DecoratedKey valueKey = cfs.getIndexKeyFor(iColumn.name(), iColumn.value());
-                            ColumnFamily indexedCf = cfs.newIndexedColumnFamily(iColumn.name());
-                            indexedCf.addColumn(new Column(key.key, ArrayUtils.EMPTY_BYTE_ARRAY, iColumn.clock()));
-                            logger.debug("adding indexed column row mutation for key {}", valueKey);
-                            Table.open(desc.ksname).applyIndexedCF(cfs.getIndexedColumnFamilyStore(iColumn.name()),
-                                                                   key,
-                                                                   valueKey,
-                                                                   indexedCf);
-                        }
-                    }
-                }
-
-                iwriter.afterAppend(key, dataPosition);
-                dataPosition = dfile.getFilePointer() + dataSize;
-                dfile.seek(dataPosition);
-                rows++;
-            }
-
-            for (byte[] column : cfs.getIndexedColumns())
-            {
-                try
-                {
-                    cfs.getIndexedColumnFamilyStore(column).forceBlockingFlush();
-                }
-                catch (ExecutionException e)
-                {
-                    throw new RuntimeException(e);
-                }
-                catch (InterruptedException e)
-                {
-                    throw new AssertionError(e);
-                }
-            }
-        }
-        finally
-        {
-            try
-            {
-                dfile.close();
-                iwriter.close();
-            }
-            catch (IOException e)
-            {
-                logger.error("Failed to close data or index file during recovery of " + desc, e);
-            }
-        }
-
-        logger.debug("estimated row count was %s of real count", ((double)estimatedRows) / rows);
-    }
-
-    /**
      * Removes the given SSTable from temporary status and opens it, rebuilding the non-essential portions of the
      * file if necessary.
      */
-    public static SSTableReader recoverAndOpen(Descriptor desc) throws IOException
+    public static SSTableReader recoverAndOpen(Descriptor desc, IRecoveryProcessor rp) throws IOException
     {
         if (!desc.isLatestVersion)
             // TODO: streaming between different versions will fail: need support for
             // recovering other versions to provide a stable streaming api
             throw new RuntimeException(String.format("Cannot recover SSTable with version %s (current version %s).",
                                                      desc.version, Descriptor.CURRENT_VERSION));
+ 
+        // check that both index and filter files are present
+        logger.debug("In recoverAndOpen with Descriptor {}", desc);
+        File ifile = new File(desc.filenameFor(SSTable.COMPONENT_INDEX));
+        File ffile = new File(desc.filenameFor(SSTable.COMPONENT_FILTER));
+        if (ifile.exists() && ffile.exists()) {
+            // nothing to do
+            return SSTableReader.open(rename(desc, SSTable.componentsFor(desc)));
+        }
+
+        // remove existing files
+        ifile.delete();
+        ffile.delete();
+        
+        // recover sstable
+        rp.recover(desc);
 
         // FIXME: once maybeRecover is recovering BMIs, it should return the recovered
         // components
-        maybeRecover(desc);
-        return SSTableReader.open(rename(desc, SSTable.componentsFor(desc)));
+        return SSTableReader.open(desc.asTemporary(false));
     }
 
     /**
      * Encapsulates writing the index and filter for an SSTable. The state of this object is not valid until it has been closed.
      */
-    static class IndexWriter
+    protected static class IndexWriter
     {
         private final BufferedRandomAccessFile indexFile;
         public final Descriptor desc;
Index: src/java/org/apache/cassandra/service/AntiEntropyService.java
===================================================================
--- src/java/org/apache/cassandra/service/AntiEntropyService.java	(revision 996701)
+++ src/java/org/apache/cassandra/service/AntiEntropyService.java	(working copy)
@@ -41,6 +41,7 @@
 import org.apache.cassandra.io.AbstractCompactedRow;
 import org.apache.cassandra.io.ICompactSerializer;
 import org.apache.cassandra.io.sstable.SSTableReader;
+import org.apache.cassandra.streaming.OperationType;
 import org.apache.cassandra.streaming.StreamContext;
 import org.apache.cassandra.streaming.StreamIn;
 import org.apache.cassandra.streaming.StreamOut;
@@ -539,14 +540,14 @@
                     protected void runMayThrow() throws Exception
                     {
                         StreamContext context = new StreamContext(request.endpoint);
-                        StreamOut.transferSSTables(context, request.cf.left, sstables, ranges);
+                        StreamOut.transferSSTables(context, request.cf.left, sstables, ranges, OperationType.AES);
                         StreamOutManager.remove(context);
                     }
                 });
                 // request ranges from the remote node
                 // FIXME: no way to block for the 'requestRanges' call to complete, or to request a
                 // particular cf: see CASSANDRA-1189
-                StreamIn.requestRanges(request.endpoint, request.cf.left, ranges);
+                StreamIn.requestRanges(request.endpoint, request.cf.left, ranges, OperationType.AES);
                 
                 // wait until streaming has completed
                 f.get();
Index: src/java/org/apache/cassandra/service/ReadResponseResolver.java
===================================================================
--- src/java/org/apache/cassandra/service/ReadResponseResolver.java	(revision 996701)
+++ src/java/org/apache/cassandra/service/ReadResponseResolver.java	(working copy)
@@ -79,12 +79,24 @@
             ReadResponse result = ReadResponse.serializer().deserialize(new DataInputStream(bufIn));
             if (result.isDigestQuery())
             {
+                if (isDigestQuery)
+                {
+                    byte[] resultDigest = result.digest();
+                    checkDigest(key, digest, resultDigest);
+                }
                 digest = result.digest();
                 isDigestQuery = true;
             }
             else
             {
-                versions.add(result.row().cf);
+                ColumnFamily cf = result.row().cf;
+                if (!FBUtilities.getLocalAddress().equals(response.getFrom()) && cf != null)
+                {
+                    cf = cf.cloneMe();
+                    cf.cleanContext(FBUtilities.getLocalAddress());
+                }
+
+                versions.add(cf);
                 endpoints.add(response.getFrom());
                 key = result.row().key;
             }
@@ -95,12 +107,8 @@
         {
             for (ColumnFamily cf : versions)
             {
-                if (!Arrays.equals(ColumnFamily.digest(cf), digest))
-                {
-                    /* Wrap the key as the context in this exception */
-                    String s = String.format("Mismatch for key %s (%s vs %s)", key, FBUtilities.bytesToHex(ColumnFamily.digest(cf)), FBUtilities.bytesToHex(digest));
-                    throw new DigestMismatchException(s);
-                }
+                byte[] resultDigest = ColumnFamily.digest(cf);
+                checkDigest(key, digest, resultDigest);
             }
         }
 
@@ -112,6 +120,16 @@
 		return new Row(key, resolved);
 	}
 
+    private void checkDigest(DecoratedKey key, byte[] digest, byte[] resultDigest) throws DigestMismatchException
+    {
+        if (!Arrays.equals(resultDigest, digest))
+        {
+            /* Wrap the key as the context in this exception */
+            String s = String.format("Mismatch for key %s (%s vs %s)", key, FBUtilities.bytesToHex(resultDigest), FBUtilities.bytesToHex(digest));
+            throw new DigestMismatchException(s);
+        }
+    }
+
     /**
      * For each row version, compare with resolved (the superset of all row versions);
      * if it is missing anything, send a mutation to the endpoint it come from.
@@ -126,6 +144,11 @@
 
             // create and send the row mutation message based on the diff
             RowMutation rowMutation = new RowMutation(table, key.key);
+
+            diffCf.cleanContext(endpoints.get(i));
+            if (diffCf.getColumnsMap().isEmpty() && !diffCf.isMarkedForDelete())
+                continue;
+
             rowMutation.add(diffCf);
             RowMutationMessage rowMutationMessage = new RowMutationMessage(rowMutation);
             Message repairMessage;
@@ -149,7 +172,7 @@
         {
             if (cf != null)
             {
-                resolved = cf.cloneMe();
+                resolved = cf.cloneMeShallow();
                 break;
             }
         }
Index: src/java/org/apache/cassandra/service/StorageProxy.java
===================================================================
--- src/java/org/apache/cassandra/service/StorageProxy.java	(revision 996701)
+++ src/java/org/apache/cassandra/service/StorageProxy.java	(working copy)
@@ -53,6 +53,7 @@
 import org.apache.cassandra.thrift.*;
 import org.apache.cassandra.utils.FBUtilities;
 import org.apache.cassandra.utils.LatencyTracker;
+import org.apache.cassandra.utils.Pair;
 import org.apache.cassandra.utils.WrappedRunnable;
 import org.apache.cassandra.db.filter.QueryFilter;
 
@@ -115,6 +116,9 @@
 
                 responseHandlers.add(responseHandler);
                 Message unhintedMessage = null;
+
+                updateDestinationByClock(consistency_level, rm, hintedEndpoints);
+                
                 for (Map.Entry<InetAddress, Collection<InetAddress>> entry : hintedEndpoints.asMap().entrySet())
                 {
                     InetAddress destination = entry.getKey();
@@ -123,6 +127,8 @@
                     if (targets.size() == 1 && targets.iterator().next().equals(destination))
                     {
                         // unhinted writes
+                        rm.updateClocks(destination);
+                        
                         if (destination.equals(FBUtilities.getLocalAddress()))
                         {
                             insertLocalMessage(rm, responseHandler);
@@ -178,6 +184,43 @@
 
     }
 
+    
+    
+    /**
+     * Update destination endpoints depending on the clock type.
+     */
+    private static void updateDestinationByClock(ConsistencyLevel consistency_level, RowMutation rm,
+            Multimap<InetAddress, InetAddress> destinationEndpoints)
+    {
+        ClockType clockType = rm.getColumnFamilies().iterator().next().getClockType();
+        if (clockType != ClockType.IncrementCounter)
+            return;
+        
+        assert ConsistencyLevel.ONE == consistency_level || ConsistencyLevel.ZERO == consistency_level: "Context-based CFs only support ConsistencyLevel.ONE or ZERO";
+        InetAddress randomDestination = pickRandomDestination(destinationEndpoints);
+        destinationEndpoints.clear();
+        destinationEndpoints.put(randomDestination, randomDestination);
+    }
+
+    /**
+     * @param endpoints potential destinations.
+     * @return one destination randomly chosen from the endpoints unless localhost is in the map, then that is returned.
+     */
+    private static InetAddress pickRandomDestination(Multimap<InetAddress, InetAddress> endpoints)
+    {
+        Set<InetAddress> destinationSet = endpoints.keySet();
+        
+        if (destinationSet.contains(FBUtilities.getLocalAddress()))
+        {
+            return FBUtilities.getLocalAddress();
+        }
+        else
+        {
+            InetAddress[] destinations = destinationSet.toArray(new InetAddress[0]);
+            return destinations[random.nextInt(destinations.length)];
+        }
+    }
+
     private static void addHintHeader(Message message, InetAddress target)
     {
         byte[] oldHint = message.getHeader(RowMutation.HINT);
Index: src/java/org/apache/cassandra/service/StorageService.java
===================================================================
--- src/java/org/apache/cassandra/service/StorageService.java	(revision 996701)
+++ src/java/org/apache/cassandra/service/StorageService.java	(working copy)
@@ -942,7 +942,7 @@
                 {
                     if (logger_.isDebugEnabled())
                         logger_.debug("Requesting from " + entry.getKey() + " ranges " + StringUtils.join(entry.getValue(), ", "));
-                    StreamIn.requestRanges(entry.getKey(), table, entry.getValue());
+                    StreamIn.requestRanges(entry.getKey(), table, entry.getValue(), OperationType.RESTORE_REPLICA_COUNT);
                 }
             }
         }
@@ -1533,7 +1533,7 @@
                     public void run()
                     {
                         // TODO each call to transferRanges re-flushes, this is potentially a lot of waste
-                        StreamOut.transferRanges(newEndpoint, table, Arrays.asList(range), callback);
+                        StreamOut.transferRanges(newEndpoint, table, Arrays.asList(range), callback, OperationType.UNBOOTSTRAP);
                     }
                 });
             }
Index: src/java/org/apache/cassandra/streaming/FileStatusHandler.java
===================================================================
--- src/java/org/apache/cassandra/streaming/FileStatusHandler.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/FileStatusHandler.java	(working copy)
@@ -26,6 +26,9 @@
 
 import org.apache.cassandra.db.Table;
 import org.apache.cassandra.io.sstable.Descriptor;
+import org.apache.cassandra.io.sstable.AESRecoveryProcessor;
+import org.apache.cassandra.io.sstable.IRecoveryProcessor;
+import org.apache.cassandra.io.sstable.IndexRecoveryProcessor;
 import org.apache.cassandra.io.sstable.SSTableReader;
 import org.apache.cassandra.io.sstable.SSTableWriter;
 import org.apache.cassandra.net.MessagingService;
@@ -50,7 +53,7 @@
         assert FileStatus.Action.DELETE == streamStatus.getAction() :
             "Unknown stream action: " + streamStatus.getAction();
 
-        addSSTable(pendingFile);
+        addSSTable(pendingFile, context);
 
         // send a StreamStatus message telling the source node it can delete this file
         if (logger.isDebugEnabled())
@@ -58,13 +61,15 @@
         MessagingService.instance.sendOneWay(streamStatus.makeStreamStatusMessage(), context.host);
     }
 
-    public static void addSSTable(PendingFile pendingFile)
+    public static void addSSTable(PendingFile pendingFile, StreamContext context)
     {
         // file was successfully streamed
         Descriptor desc = pendingFile.desc;
         try
         {
-            SSTableReader sstable = SSTableWriter.recoverAndOpen(pendingFile.desc);
+            // right now we only need to do this differently for AES operations
+            IRecoveryProcessor rp = pendingFile.type == OperationType.AES ? new AESRecoveryProcessor(context.host) : IndexRecoveryProcessor.instance();
+            SSTableReader sstable = SSTableWriter.recoverAndOpen(pendingFile.desc, rp);
             Table.open(desc.ksname).getColumnFamilyStore(desc.cfname).addSSTable(sstable);
             logger.info("Streaming added " + sstable);
         }
Index: src/java/org/apache/cassandra/streaming/IncomingStreamReader.java
===================================================================
--- src/java/org/apache/cassandra/streaming/IncomingStreamReader.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/IncomingStreamReader.java	(working copy)
@@ -106,7 +106,7 @@
             handleFileStatus(FileStatus.Action.DELETE);
         else
         {
-            FileStatusHandler.addSSTable(pendingFile);
+            FileStatusHandler.addSSTable(pendingFile, context);
             StreamInManager.get(context).finishAndRequestNext(lastFile);
         }
     }
Index: src/java/org/apache/cassandra/streaming/OperationType.java
===================================================================
--- src/java/org/apache/cassandra/streaming/OperationType.java	(revision 0)
+++ src/java/org/apache/cassandra/streaming/OperationType.java	(revision 0)
@@ -0,0 +1,26 @@
+ /**
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+package org.apache.cassandra.streaming;
+
+/**
+ * What kind of streaming operation?
+ */
+public enum OperationType
+{
+    AES, BOOTSTRAP, UNBOOTSTRAP, RESTORE_REPLICA_COUNT;
+}
Index: src/java/org/apache/cassandra/streaming/PendingFile.java
===================================================================
--- src/java/org/apache/cassandra/streaming/PendingFile.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/PendingFile.java	(working copy)
@@ -51,17 +51,19 @@
     public final Descriptor desc;
     public final String component;
     public final List<Pair<Long,Long>> sections;
-
+    public final OperationType type;
+   
     public PendingFile(Descriptor desc, PendingFile pf)
     {
-        this(desc, pf.component, pf.sections);
+        this(desc, pf.component, pf.sections, pf.type);
     }
 
-    public PendingFile(Descriptor desc, String component, List<Pair<Long,Long>> sections)
+    public PendingFile(Descriptor desc, String component, List<Pair<Long,Long>> sections, OperationType type)
     {
         this.desc = desc;
         this.component = component;
         this.sections = sections;
+        this.type = type;
     }
 
     public String getFilename()
@@ -99,6 +101,7 @@
             {
                 dos.writeLong(section.left); dos.writeLong(section.right);
             }
+            dos.writeUTF(sc.type.name());
         }
 
         public PendingFile deserialize(DataInputStream dis) throws IOException
@@ -109,7 +112,8 @@
             List<Pair<Long,Long>> sections = new ArrayList<Pair<Long,Long>>(count);
             for (int i = 0; i < count; i++)
                 sections.add(new Pair<Long,Long>(Long.valueOf(dis.readLong()), Long.valueOf(dis.readLong())));
-            return new PendingFile(desc, component, sections);
+            OperationType type = OperationType.valueOf(dis.readUTF());
+            return new PendingFile(desc, component, sections, type);
         }
     }
 }
Index: src/java/org/apache/cassandra/streaming/StreamIn.java
===================================================================
--- src/java/org/apache/cassandra/streaming/StreamIn.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/StreamIn.java	(working copy)
@@ -45,7 +45,7 @@
     /**
      * Request ranges to be transferred from source to local node
      */
-    public static void requestRanges(InetAddress source, String tableName, Collection<Range> ranges)
+    public static void requestRanges(InetAddress source, String tableName, Collection<Range> ranges, OperationType type)
     {
         assert ranges.size() > 0;
 
@@ -53,7 +53,7 @@
             logger.debug("Requesting from {} ranges {}", source, StringUtils.join(ranges, ", "));
         StreamContext context = new StreamContext(source);
         StreamInManager.get(context);
-        Message message = new StreamRequestMessage(FBUtilities.getLocalAddress(), ranges, tableName, context.sessionId).makeMessage();
+        Message message = new StreamRequestMessage(FBUtilities.getLocalAddress(), ranges, tableName, context.sessionId, type).makeMessage();
         MessagingService.instance.sendOneWay(message, source);
     }
 
Index: src/java/org/apache/cassandra/streaming/StreamOut.java
===================================================================
--- src/java/org/apache/cassandra/streaming/StreamOut.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/StreamOut.java	(working copy)
@@ -62,7 +62,7 @@
     /**
      * Split out files for all tables on disk locally for each range and then stream them to the target endpoint.
     */
-    public static void transferRanges(InetAddress target, String tableName, Collection<Range> ranges, Runnable callback)
+    public static void transferRanges(InetAddress target, String tableName, Collection<Range> ranges, Runnable callback, OperationType type)
     {
         assert ranges.size() > 0;
         
@@ -76,7 +76,7 @@
         {
             Table table = flushSSTable(tableName);
             // send the matching portion of every sstable in the keyspace
-            transferSSTables(context, tableName, table.getAllSSTables(), ranges);
+            transferSSTables(context, tableName, table.getAllSSTables(), ranges, type);
         }
         catch (IOException e)
         {
@@ -120,7 +120,7 @@
     /**
      * Split out files for all tables on disk locally for each range and then stream them to the target endpoint.
     */
-    public static void transferRangesForRequest(StreamContext context, String tableName, Collection<Range> ranges, Runnable callback)
+    public static void transferRangesForRequest(StreamContext context, String tableName, Collection<Range> ranges, Runnable callback, OperationType type)
     {
         assert ranges.size() > 0;
 
@@ -130,7 +130,7 @@
         {
             Table table = flushSSTable(tableName);
             // send the matching portion of every sstable in the keyspace
-            transferSSTablesForRequest(context, tableName, table.getAllSSTables(), ranges);
+            transferSSTablesForRequest(context, tableName, table.getAllSSTables(), ranges, type);
         }
         catch (IOException e)
         {
@@ -144,9 +144,9 @@
     /**
      * Transfers matching portions of a group of sstables from a single table to the target endpoint.
      */
-    public static void transferSSTables(StreamContext context, String table, Collection<SSTableReader> sstables, Collection<Range> ranges) throws IOException
+    public static void transferSSTables(StreamContext context, String table, Collection<SSTableReader> sstables, Collection<Range> ranges, OperationType type) throws IOException
     {
-        List<PendingFile> pending = createPendingFiles(sstables, ranges);
+        List<PendingFile> pending = createPendingFiles(sstables, ranges, type);
 
         if (pending.size() > 0)
         {
@@ -166,9 +166,9 @@
      * Transfers the first file for matching portions of a group of sstables and appends a list of other files
      * to the header for the requesting destination to take control of the rest of the transfers
      */
-    private static void transferSSTablesForRequest(StreamContext context, String table, Collection<SSTableReader> sstables, Collection<Range> ranges) throws IOException
+    private static void transferSSTablesForRequest(StreamContext context, String table, Collection<SSTableReader> sstables, Collection<Range> ranges, OperationType type) throws IOException
     {
-        List<PendingFile> pending = createPendingFiles(sstables, ranges);
+        List<PendingFile> pending = createPendingFiles(sstables, ranges, type);
         if (pending.size() > 0)
         {
             StreamHeader header = new StreamHeader(context.sessionId, pending.get(0), pending, false);
@@ -191,7 +191,7 @@
     }
 
     // called prior to sending anything.
-    private static List<PendingFile> createPendingFiles(Collection<SSTableReader> sstables, Collection<Range> ranges)
+    private static List<PendingFile> createPendingFiles(Collection<SSTableReader> sstables, Collection<Range> ranges, OperationType type)
     {
         List<PendingFile> pending = new ArrayList<PendingFile>();
         for (SSTableReader sstable : sstables)
@@ -200,7 +200,7 @@
             List<Pair<Long,Long>> sections = sstable.getPositionsForRanges(ranges);
             if (sections.isEmpty())
                 continue;
-            pending.add(new PendingFile(desc, SSTable.COMPONENT_DATA, sections));
+            pending.add(new PendingFile(desc, SSTable.COMPONENT_DATA, sections, type));
         }
         logger.info("Stream context metadata {}, {} sstables.", pending, sstables.size());
         return pending;
Index: src/java/org/apache/cassandra/streaming/StreamRequestMessage.java
===================================================================
--- src/java/org/apache/cassandra/streaming/StreamRequestMessage.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/StreamRequestMessage.java	(working copy)
@@ -66,13 +66,15 @@
     // if these are specified, file shoud not be.
     protected final Collection<Range> ranges;
     protected final String table;
+    protected final OperationType type;
 
-    StreamRequestMessage(InetAddress target, Collection<Range> ranges, String table, long sessionId)
+    StreamRequestMessage(InetAddress target, Collection<Range> ranges, String table, long sessionId, OperationType type)
     {
         this.target = target;
         this.ranges = ranges;
         this.table = table;
         this.sessionId = sessionId;
+        this.type = type;
         file = null;
     }
 
@@ -81,6 +83,7 @@
         this.target = target;
         this.file = file;
         this.sessionId = sessionId;
+        this.type = file.type;
         ranges = null;
         table = null;
     }
@@ -114,6 +117,7 @@
                 sb.append(range);
                 sb.append(" ");
             }
+            sb.append(type);
         }
         else
         {
@@ -142,6 +146,7 @@
                 {
                     AbstractBounds.serializer().serialize(range, dos);
                 }
+                dos.writeUTF(srm.type.name());
             }
         }
 
@@ -164,7 +169,8 @@
                 {
                     ranges.add((Range) AbstractBounds.serializer().deserialize(dis));
                 }
-                return new StreamRequestMessage(target, ranges, table, sessionId);
+                OperationType type = OperationType.valueOf(dis.readUTF());
+                return new StreamRequestMessage(target, ranges, table, sessionId, type);
             }
         }
     }
Index: src/java/org/apache/cassandra/streaming/StreamRequestVerbHandler.java
===================================================================
--- src/java/org/apache/cassandra/streaming/StreamRequestVerbHandler.java	(revision 996701)
+++ src/java/org/apache/cassandra/streaming/StreamRequestVerbHandler.java	(working copy)
@@ -62,7 +62,7 @@
             else
             {
                 // range request.
-                StreamOut.transferRangesForRequest(new StreamContext(message.getFrom(), srm.sessionId), srm.table, srm.ranges, null);
+                StreamOut.transferRangesForRequest(new StreamContext(message.getFrom(), srm.sessionId), srm.table, srm.ranges, null, srm.type);
             }
         }
         catch (IOException ex)
Index: src/java/org/apache/cassandra/thrift/CassandraServer.java
===================================================================
--- src/java/org/apache/cassandra/thrift/CassandraServer.java	(revision 996701)
+++ src/java/org/apache/cassandra/thrift/CassandraServer.java	(working copy)
@@ -47,7 +47,6 @@
 import org.apache.cassandra.locator.AbstractReplicationStrategy;
 import org.apache.cassandra.config.*;
 import org.apache.cassandra.db.*;
-import org.apache.cassandra.db.ColumnFamily;
 import org.apache.cassandra.db.clock.AbstractReconciler;
 import org.apache.cassandra.db.clock.TimestampReconciler;
 import org.apache.cassandra.db.filter.QueryPath;
@@ -367,8 +366,8 @@
         ThriftValidation.validateKey(key);
         ThriftValidation.validateColumnParent(clientState.getKeyspace(), column_parent);
         ThriftValidation.validateColumn(clientState.getKeyspace(), column_parent, column);
-        IClock cassandra_clock = ThriftValidation.validateClock(column.clock);
-
+        IClock cassandra_clock = ThriftValidation.validateClock(clientState.getKeyspace(), column_parent.getColumn_family(), column.clock);
+        
         RowMutation rm = new RowMutation(clientState.getKeyspace(), key);
         try
         {
@@ -411,6 +410,42 @@
         doInsert(consistency_level, rowMutations);
     }
 
+    public void increment(Map<byte[], Map<String, List<Mutation>>> mutation_map)
+            throws InvalidRequestException, UnavailableException, TimedOutException, TException
+    {
+        if (logger.isDebugEnabled())
+            logger.debug("increment");
+
+        clientState.hasKeyspaceAccess(Permission.WRITE_VALUE);
+        KSMetaData ksm = DatabaseDescriptor.getTableDefinition(clientState.getKeyspace());
+        
+        List<RowMutation> rowMutations = new ArrayList<RowMutation>();
+        for (Map.Entry<byte[], Map<String, List<Mutation>>> mutationEntry: mutation_map.entrySet())
+        {
+            byte[] key = mutationEntry.getKey();
+            ThriftValidation.validateKey(key);
+            
+            Map<String, List<Mutation>> columnFamilyToMutations = mutationEntry.getValue();
+            for (Map.Entry<String, List<Mutation>> columnFamilyMutations : columnFamilyToMutations.entrySet())
+            {
+                String cfName = columnFamilyMutations.getKey();
+                CFMetaData cf = ksm.cfMetaData().get(cfName);
+                if (cf == null || cf.clockType != ClockType.IncrementCounter) {
+                    throw new InvalidRequestException("Invalid column family, cannot increment: " + cfName);
+                }
+                
+                for (Mutation mutation : columnFamilyMutations.getValue())
+                {
+                    ThriftValidation.validateMutation(clientState.getKeyspace(), cfName, mutation);
+                }
+            }
+            rowMutations.add(RowMutation.getRowMutationFromMutations(clientState.getKeyspace(), key, columnFamilyToMutations));
+        }
+
+        // TODO fixed at consistency level one until CASSANDRA-1397
+        doInsert(ConsistencyLevel.ONE, rowMutations);
+    }
+    
     public void remove(byte[] key, ColumnPath column_path, Clock clock, ConsistencyLevel consistency_level)
     throws InvalidRequestException, UnavailableException, TimedOutException
     {
@@ -422,7 +457,7 @@
         ThriftValidation.validateKey(key);
         ThriftValidation.validateColumnPathOrParent(clientState.getKeyspace(), column_path);
 
-        IClock cassandra_clock = ThriftValidation.validateClock(clock);
+        IClock cassandra_clock = ThriftValidation.validateClock(clientState.getKeyspace(), column_path.getColumn_family(), clock);
 
         RowMutation rm = new RowMutation(clientState.getKeyspace(), key);
         rm.delete(new QueryPath(column_path), cassandra_clock);
Index: src/java/org/apache/cassandra/thrift/ThriftValidation.java
===================================================================
--- src/java/org/apache/cassandra/thrift/ThriftValidation.java	(revision 996701)
+++ src/java/org/apache/cassandra/thrift/ThriftValidation.java	(working copy)
@@ -21,6 +21,17 @@
  */
 
 import java.util.Arrays;
+import org.apache.commons.lang.ArrayUtils;
+
+import org.apache.cassandra.db.IncrementCounterClock;
+import org.apache.cassandra.db.KeyspaceNotDefinedException;
+import org.apache.cassandra.db.ColumnFamily;
+import org.apache.cassandra.db.IColumn;
+import org.apache.cassandra.db.ColumnFamilyType;
+import org.apache.cassandra.db.IClock;
+import org.apache.cassandra.db.TimestampClock;
+import org.apache.cassandra.db.marshal.AbstractType;
+import org.apache.cassandra.db.marshal.MarshalException;
 import java.util.Comparator;
 import java.util.Set;
 
@@ -212,7 +223,8 @@
         if (cosc.column != null)
         {
             validateTtl(cosc.column);
-            validateClock(cosc.column.clock);
+            IClock clock = validateClock(keyspace, cfName, cosc.column.clock);
+            validateValueByClock(cosc.column.value, clock);
             ThriftValidation.validateColumnPath(keyspace, new ColumnPath(cfName).setSuper_column(null).setColumn(cosc.column.name));
         }
 
@@ -221,7 +233,8 @@
             for (Column c : cosc.super_column.columns)
             {
                 validateTtl(c);
-                validateClock(c.clock);
+                IClock clock = validateClock(keyspace, cfName, c.clock);
+                validateValueByClock(c.value, clock);
                 ThriftValidation.validateColumnPath(keyspace, new ColumnPath(cfName).setSuper_column(cosc.super_column.name).setColumn(c.name));
             }
         }
@@ -240,15 +253,63 @@
         assert column.isSetTtl() || column.ttl == 0;
     }
 
-    public static IClock validateClock(Clock clock) throws InvalidRequestException
+    /**
+     * Fetch the clock type for the provided column family and check that we have all the 
+     * information required to instantiate.
+     */
+    public static IClock validateClock(String keyspace, String cfName, Clock clock) throws InvalidRequestException
     {
-        if (clock.isSetTimestamp())
+        ClockType clockType = DatabaseDescriptor.getClockType(keyspace, cfName);
+        if (clockType == null)
+            throw new InvalidRequestException("No clock found for " + keyspace + " " + cfName);
+        
+        switch (clockType)
         {
+        case Timestamp:
+            if (!clock.isSetTimestamp())
+            {
+                throw new InvalidRequestException("No timestamp set, despite timestamp clock being used: " + keyspace + " " + cfName);
+            }
             return new TimestampClock(clock.getTimestamp());
+        case IncrementCounter:
+            return new IncrementCounterClock();
+        default:
+            throw new InvalidRequestException("Invalid clock type for " + keyspace + " " + cfName);
         }
-        throw new InvalidRequestException("Clock must have one a timestamp");
     }
 
+    /**
+     * Check that the value is valid for the clock specified.
+     * For example the increment counter cannot accept negative values.
+     * @param value Value to validate.
+     * @param cassandraClock Clock to check by.
+     * @throws InvalidRequestException If the value is invalid this exception is thrown.
+     */
+    public static void validateValueByClock(byte[] value, IClock cassandraClock) throws InvalidRequestException
+    {
+        switch (cassandraClock.type())
+        {
+            case IncrementCounter:
+                try
+                {
+                    long delta = FBUtilities.byteArrayToLong(value);
+                    if (delta < 0)
+                    {
+                        throw new InvalidRequestException("Value must be positive when using an increment counter");
+                    }
+                }
+                catch (IllegalArgumentException e)
+                {
+                    throw new InvalidRequestException("Value is not a valid long delta: " + e.getMessage());
+                }
+                break;
+            case Timestamp:
+            default:
+                return; //nothing to check
+        }
+        
+    }
+    
     public static void validateMutation(String keyspace, String cfName, Mutation mut)
             throws InvalidRequestException
     {
Index: src/java/org/apache/cassandra/utils/FBUtilities.java
===================================================================
--- src/java/org/apache/cassandra/utils/FBUtilities.java	(revision 996701)
+++ src/java/org/apache/cassandra/utils/FBUtilities.java	(working copy)
@@ -41,6 +41,7 @@
 import org.slf4j.Logger;
 import org.slf4j.LoggerFactory;
 
+import org.apache.commons.lang.ArrayUtils;
 import com.sun.jna.Native;
 import org.apache.cassandra.config.ConfigurationException;
 import org.apache.cassandra.config.DatabaseDescriptor;
@@ -158,21 +159,74 @@
         return new Pair(midpoint, remainder);
     }
 
+    /**
+     * Copy bytes from int into bytes starting from offset.
+     * @param bytes Target array
+     * @param offset Offset into the array
+     * @param i Value to write
+     */
+    public static void copyIntoBytes(byte[] bytes, int offset, int i)
+    {
+        bytes[offset]   = (byte)( ( i >>> 24 ) & 0xFF );
+        bytes[offset+1] = (byte)( ( i >>> 16 ) & 0xFF );
+        bytes[offset+2] = (byte)( ( i >>> 8  ) & 0xFF );
+        bytes[offset+3] = (byte)(   i          & 0xFF );
+    }
+
+    /**
+     * Copy bytes from long into bytes starting from offset.
+     * @param bytes Target array
+     * @param offset Offset into the array
+     * @param l Value to write
+     */
+    public static void copyIntoBytes(byte[] bytes, int offset, long l)
+    {
+        bytes[offset]   = (byte)( ( l >>> 56 ) & 0xFF );
+        bytes[offset+1] = (byte)( ( l >>> 48 ) & 0xFF );
+        bytes[offset+2] = (byte)( ( l >>> 40 ) & 0xFF );
+        bytes[offset+3] = (byte)( ( l >>> 32 ) & 0xFF );
+        bytes[offset+4] = (byte)( ( l >>> 24 ) & 0xFF );
+        bytes[offset+5] = (byte)( ( l >>> 16 ) & 0xFF );
+        bytes[offset+6] = (byte)( ( l >>> 8  ) & 0xFF );
+        bytes[offset+7] = (byte)(   l          & 0xFF );
+    }
+
+    /**
+     * @param i Write this int to an array
+     * @return Four byte array containing the int
+     */
     public static byte[] toByteArray(int i)
     {
         byte[] bytes = new byte[4];
-        bytes[0] = (byte)( ( i >>> 24 ) & 0xFF);
-        bytes[1] = (byte)( ( i >>> 16 ) & 0xFF);
-        bytes[2] = (byte)( ( i >>> 8 ) & 0xFF);
-        bytes[3] = (byte)( i & 0xFF );
+        copyIntoBytes(bytes, 0, i);
         return bytes;
     }
 
+    /**
+     * @param l Write this long to an array
+     * @return Four byte array containing the long
+     */
+    public static byte[] toByteArray(long l)
+    {
+        byte[] bytes = new byte[8];
+        copyIntoBytes(bytes, 0, l);
+        return bytes;
+    }
+
+    /**
+     * @param bytes A byte array containing a serialized integer.
+     * @return The integer value contained in the byte array.
+     */
     public static int byteArrayToInt(byte[] bytes)
     {
     	return byteArrayToInt(bytes, 0);
     }
 
+    /**
+     * @param bytes A byte array containing a serialized integer.
+     * @param offset Start position of the integer in the array.
+     * @return The integer value contained in the byte array.
+     */
     public static int byteArrayToInt(byte[] bytes, int offset)
     {
         if ( bytes.length - offset < 4 )
@@ -188,6 +242,35 @@
         return n;
     }
 
+    /**
+     * @param bytes A byte array containing a serialized long.
+     * @return The long value contained in the byte array.
+     */
+    public static long byteArrayToLong(byte[] bytes)
+    {
+        return byteArrayToLong(bytes, 0);
+    }
+
+    /**
+     * @param bytes A byte array containing a serialized long.
+     * @param offset Start position of the long in the array.
+     * @return The long value contained in the byte array.
+     */
+    public static long byteArrayToLong(byte[] bytes, int offset)
+    {
+        if ( bytes.length - offset < 8 )
+        {
+            throw new IllegalArgumentException("A long must be 8 bytes in size.");
+        }
+        long n = 0;
+        for ( int i = 0; i < 8; ++i )
+        {
+            n <<= 8;
+            n |= bytes[offset + i] & 0xFF;
+        }
+        return n;
+    }
+
     public static int compareByteArrays(byte[] bytes1, byte[] bytes2){
         if(null == bytes1){
             if(null == bytes2) return 0;
@@ -208,6 +291,38 @@
     }
 
     /**
+     * Compare two byte[] at specified offsets for length. Compares the non equal bytes as unsigned.
+     * @param bytes1 First array to compare.
+     * @param offset1 Position to start the comparison at in the first array.
+     * @param bytes2 Second array to compare.
+     * @param offset2 Position to start the comparison at in the second array.
+     * @param length How many bytes to compare?
+     * @return -1 if byte1 is less than byte2, 1 if byte2 is less than byte1 or 0 if equal.
+     */
+    public static int compareByteSubArrays(byte[] bytes1, int offset1, byte[] bytes2, int offset2, int length)
+    {
+        if ( null == bytes1 )
+        {
+            if ( null == bytes2) return 0;
+            else return -1;
+        }
+        if (null == bytes2 ) return 1;
+
+        assert bytes1.length >= (offset1 + length) : "The first byte array isn't long enough for the specified offset and length.";
+        assert bytes2.length >= (offset2 + length) : "The second byte array isn't long enough for the specified offset and length.";
+        for ( int i = 0; i < length; i++ )
+        {
+            byte byte1 = bytes1[offset1+i];
+            byte byte2 = bytes2[offset2+i];
+            if ( byte1 == byte2 )
+                continue;
+            // compare non-equal bytes as unsigned
+            return (byte1 & 0xFF) < (byte2 & 0xFF) ? -1 : 1;
+        }
+        return 0;
+    }
+
+    /**
      * @return The bitwise XOR of the inputs. The output will be the same length as the
      * longer input, but if either input is null, the output will be null.
      */
@@ -484,11 +599,41 @@
         return Charsets.UTF_8.newDecoder().decode(ByteBuffer.wrap(bytes)).toString();
     }
 
-    public static byte[] toByteArray(long n)
+    /** 
+     * Thin wrapper around byte[] to provide meaningful equals() and hashCode() operations
+     * caveat: assumed that wrapped byte[] will not be modified
+     */
+    public static final class ByteArrayWrapper
     {
-        byte[] bytes = new byte[8];
-        ByteBuffer.wrap(bytes).putLong(n);
-        return bytes;
+        public final byte[] data;
+
+        public ByteArrayWrapper(byte[] data)
+        {
+            if ( null == data )
+            {
+                throw new NullPointerException();
+            }
+            this.data = data;
+        }
+
+        public boolean equals(Object other)
+        {
+            if ( !( other instanceof ByteArrayWrapper ) )
+            {
+                return false;
+            }
+            return Arrays.equals(data, ((ByteArrayWrapper)other).data);
+        }
+
+        public int hashCode()
+        {
+            return Arrays.hashCode(data);
+        }
+
+        public String toString()
+        {
+            return ArrayUtils.toString(data);
+        }
     }
 
     public static String resourceToFile(String filename) throws ConfigurationException
Index: test/conf/cassandra.yaml
===================================================================
--- test/conf/cassandra.yaml	(revision 996701)
+++ test/conf/cassandra.yaml	(working copy)
@@ -69,6 +69,16 @@
         - name: Super4
           column_type: Super
           compare_subcolumns_with: UTF8Type
+          
+        - name: IncrementCounter1
+          column_type: Standard
+          clock_type: IncrementCounter
+          reconciler: IncrementCounterReconciler 
+          
+        - name: SuperIncrementCounter1
+          column_type: Super
+          clock_type: IncrementCounter
+          reconciler: IncrementCounterReconciler
 
         - name: Indexed1
           column_metadata:
@@ -124,5 +134,9 @@
       replica_placement_strategy: org.apache.cassandra.locator.SimpleStrategy
       replication_factor: 2
       column_families:
-
         - name: Standard1
+
+        - name: IncrementCounter1
+          column_type: Standard
+          clock_type: IncrementCounter
+          reconciler: IncrementCounterReconciler
\ No newline at end of file
Index: test/system/__init__.py
===================================================================
--- test/system/__init__.py	(revision 996701)
+++ test/system/__init__.py	(working copy)
@@ -153,6 +153,8 @@
             Cassandra.CfDef('Keyspace1', 'Super2', column_type='Super', subcomparator_type='LongType'), 
             Cassandra.CfDef('Keyspace1', 'Super3', column_type='Super', subcomparator_type='LongType'), 
             Cassandra.CfDef('Keyspace1', 'Super4', column_type='Super', subcomparator_type='UTF8Type'),
+            Cassandra.CfDef('Keyspace1', 'IncrementCounter1', clock_type='IncrementCounter', reconciler='IncrementCounterReconciler'),
+            Cassandra.CfDef('Keyspace1', 'SuperIncrementCounter1', column_type='Super', clock_type='IncrementCounter', reconciler='IncrementCounterReconciler'),
             Cassandra.CfDef('Keyspace1', 'Indexed1', column_metadata=[Cassandra.ColumnDef('birthdate', 'LongType', Cassandra.IndexType.KEYS, 'birthdate')]),
         ])
 
Index: test/system/test_thrift_server.py
===================================================================
--- test/system/test_thrift_server.py	(revision 996701)
+++ test/system/test_thrift_server.py	(working copy)
@@ -1373,6 +1373,93 @@
             client.describe_ring('system')
         _expect_exception(req, InvalidRequestException)
 
+    def test_incr_standard_insert(self):
+        d1 = 234
+        d2 = 52345
+        _set_keyspace('Keyspace1')
+        cf = 'IncrementCounter1'
+        key = 'key1'
+        values = [struct.pack('>q', d1), struct.pack('>q', d2)]
+        
+        mutations = [Mutation(ColumnOrSuperColumn(Column('c1', v, Clock()))) for v in values]
+        mutation_map = dict({cf: mutations})
+        keyed_mutations = dict({key: mutation_map})
+        
+        client.increment(keyed_mutations)
+        client.increment(keyed_mutations)
+        
+        time.sleep(0.1)
+        
+        rv = client.get(key, ColumnPath(cf, column='c1'), ConsistencyLevel.ONE)
+        assert struct.unpack('>q', rv.column.value)[0] == (d1+d1+d2+d2)
+
+    def test_incr_super_insert(self):
+        d1 = 234
+        d2 = 52345
+        _set_keyspace('Keyspace1')
+        cf = 'SuperIncrementCounter1'
+        key = 'key1'
+        values = [struct.pack('>q', d1), struct.pack('>q', d2)]
+        
+        columns = [Column('c1', v, Clock()) for v in values]
+        
+        mutations = [Mutation(ColumnOrSuperColumn(super_column=SuperColumn('sc1', columns)))]
+        mutation_map = dict({cf: mutations})
+        keyed_mutations = dict({key: mutation_map})
+        
+        client.increment(keyed_mutations)
+        client.increment(keyed_mutations)
+        
+        time.sleep(0.1)
+        
+        rv = client.get(key, ColumnPath(cf, 'sc1', 'c1'), ConsistencyLevel.ONE)
+        assert struct.unpack('>q', rv.column.value)[0] == (d1+d1+d2+d2)
+
+    def test_incr_standard_remove(self):
+        d1 = 234
+        d2 = 52345
+        _set_keyspace('Keyspace1')
+        cf = 'IncrementCounter1'
+        key = 'key1'
+        values = [struct.pack('>q', d1), struct.pack('>q', d2)]
+        
+        mutations = [Mutation(ColumnOrSuperColumn(Column('c1', v, Clock()))) for v in values]
+        mutation_map = dict({cf: mutations})
+        keyed_mutations = dict({key: mutation_map})
+        
+        client.increment(keyed_mutations)
+        client.increment(keyed_mutations)
+        
+        time.sleep(0.1)
+        
+        client.remove('key1', ColumnPath(cf), Clock(), ConsistencyLevel.ONE)
+        time.sleep(0.1)
+        _assert_no_columnpath('key1', ColumnPath('IncrementCounter1', column='c1'))
+
+    def test_incr_super_remove(self):
+        d1 = 234
+        d2 = 52345
+        _set_keyspace('Keyspace1')
+        cf = 'SuperIncrementCounter1'
+        key = 'key1'
+        values = [struct.pack('>q', d1), struct.pack('>q', d2)]
+        
+        columns = [Column('c1', v, Clock()) for v in values]
+        
+        mutations = [Mutation(ColumnOrSuperColumn(super_column=SuperColumn('sc1', columns)))]
+        mutation_map = dict({cf: mutations})
+        keyed_mutations = dict({key: mutation_map})
+        
+        client.increment(keyed_mutations)
+        client.increment(keyed_mutations)
+        
+        time.sleep(0.1)
+        
+        client.remove('key1', ColumnPath('SuperIncrementCounter1', 'sc1'), Clock(), ConsistencyLevel.ONE)
+        time.sleep(0.1)
+        _assert_no_columnpath('key1', ColumnPath('SuperIncrementCounter1', 'sc1', 'c1'))
+  
+
     def test_index_scan(self):
         _set_keyspace('Keyspace1')
         client.insert('key1', ColumnParent('Indexed1'), Column('birthdate', _i64(1), Clock(0)), ConsistencyLevel.ONE)
@@ -1403,7 +1490,6 @@
         assert result[0].key == 'key3'
         assert len(result[0].columns) == 2, result[0].columns
         
-        
 
 class TestTruncate(ThriftTester):
     def test_truncate(self):
Index: test/unit/org/apache/cassandra/Util.java
===================================================================
--- test/unit/org/apache/cassandra/Util.java	(revision 996701)
+++ test/unit/org/apache/cassandra/Util.java	(working copy)
@@ -106,6 +106,27 @@
         return cfStore.getColumnFamily(QueryFilter.getIdentityFilter(key, new QueryPath(cfName)));
     }
 
+    public static byte[] concatByteArrays(byte[] first, byte[]... remaining)
+    {
+        int length = first.length;
+        for (byte[] array : remaining)
+        {
+            length += array.length;
+        }
+
+        byte[] result = new byte[length];
+        System.arraycopy(first, 0, result, 0, first.length);
+        int offset = first.length;
+
+        for (byte[] array : remaining)
+        {
+            System.arraycopy(array, 0, result, offset, array.length);
+            offset += array.length;
+        }
+
+        return result;
+    }
+        
     public static ColumnFamily cloneAndRemoveDeleted(ColumnFamily cf, int gcBefore)
     {
         return ColumnFamilyStore.removeDeleted(cf.cloneMe(), gcBefore);
Index: test/unit/org/apache/cassandra/db/IncrementCounterClockTest.java
===================================================================
--- test/unit/org/apache/cassandra/db/IncrementCounterClockTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/db/IncrementCounterClockTest.java	(revision 0)
@@ -0,0 +1,332 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db;
+
+import java.io.ByteArrayInputStream;
+import java.io.DataInputStream;
+import java.io.IOException;
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.junit.Test;
+
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.clock.IncrementCounterContext;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class IncrementCounterClockTest
+{
+    private static final IncrementCounterContext icc = new IncrementCounterContext();
+
+    private static final int HEADER_LENGTH;
+
+    private static final int idLength;
+    private static final int countLength;
+
+    private static final int stepLength;
+
+    static
+    {
+        HEADER_LENGTH = icc.HEADER_LENGTH;
+
+        idLength      = 4; // size of int
+        countLength   = 8; // size of long
+
+        stepLength    = idLength + countLength;
+    }
+
+    @Test
+    public void testUpdate() throws UnknownHostException
+    {
+        IncrementCounterClock clock;
+
+        // note: updates are in-place
+        clock = new IncrementCounterClock();
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        assert clock.context().length == (HEADER_LENGTH + stepLength);
+
+        assert  1 == FBUtilities.byteArrayToInt( clock.context(), HEADER_LENGTH + 0*stepLength);
+        assert 1L == FBUtilities.byteArrayToLong(clock.context(), HEADER_LENGTH + 0*stepLength + idLength);
+
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(2)), 3L);
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(2)), 2L);
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(2)), 9L);
+
+        assert clock.context().length == (HEADER_LENGTH + 2 * stepLength);
+
+        assert   2 == FBUtilities.byteArrayToInt(clock.context(),  HEADER_LENGTH + 0*stepLength);
+        assert 14L == FBUtilities.byteArrayToLong(clock.context(), HEADER_LENGTH + 0*stepLength + idLength);
+
+        assert  1 == FBUtilities.byteArrayToInt(clock.context(),  HEADER_LENGTH + 1*stepLength);
+        assert 1L == FBUtilities.byteArrayToLong(clock.context(), HEADER_LENGTH + 1*stepLength + idLength);
+    }
+
+    @Test
+    public void testCompare() throws UnknownHostException
+    {
+        IncrementCounterClock clock;
+        IncrementCounterClock other;
+
+        // greater than
+        clock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(10L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1L)
+            ));
+        other = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1L)
+            ));
+
+        assert clock.compare(other) == IClock.ClockRelationship.GREATER_THAN;
+
+        // equal
+        clock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1L)
+            ));
+        other = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1L)
+            ));
+
+        assert clock.compare(other) == IClock.ClockRelationship.EQUAL;
+
+        // less than
+        clock = new IncrementCounterClock(Util.concatByteArrays(FBUtilities.toByteArray(0L), FBUtilities.toByteArray(0L)));
+        other = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1L)
+            ));
+
+        assert clock.compare(other) == IClock.ClockRelationship.LESS_THAN;
+
+        // disjoint: not possible
+    }
+
+    @Test
+    public void testDiff() throws UnknownHostException
+    {
+        IncrementCounterClock clock;
+        IncrementCounterClock other;
+      
+        // greater than
+        clock = new IncrementCounterClock(createEmptyContext());
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        other = new IncrementCounterClock(createEmptyContext());
+
+        assert clock.diff(other) == IClock.ClockRelationship.GREATER_THAN;
+
+        // equal
+        clock = new IncrementCounterClock(createEmptyContext());
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        other = new IncrementCounterClock(createEmptyContext());
+        other.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        assert clock.diff(other) == IClock.ClockRelationship.EQUAL;
+
+        // less than
+        clock = new IncrementCounterClock(createEmptyContext());
+
+        other = new IncrementCounterClock(createEmptyContext());
+        other.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+        other.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        assert clock.diff(other) == IClock.ClockRelationship.LESS_THAN;
+
+        // disjoint
+        clock = new IncrementCounterClock(createEmptyContext());
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+        clock.update(InetAddress.getByAddress(FBUtilities.toByteArray(2)), 1L);
+
+        other = new IncrementCounterClock(createEmptyContext());
+        other.update(InetAddress.getByAddress(FBUtilities.toByteArray(9)), 1L);
+        other.update(InetAddress.getByAddress(FBUtilities.toByteArray(1)), 1L);
+
+        assert clock.diff(other) == IClock.ClockRelationship.DISJOINT;
+    }
+
+    private byte[] createEmptyContext()
+    {
+        byte[] context = new byte[HEADER_LENGTH];
+        FBUtilities.copyIntoBytes(context, 0, 0);
+        FBUtilities.copyIntoBytes(context, IncrementCounterContext.TIMESTAMP_LENGTH, 0);
+        return context;
+    }
+
+    @Test
+    public void testGetSuperset()
+    {
+        // normal list
+        List<IClock> clocks = new LinkedList<IClock>();
+        clocks.add(new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(32L)
+            )));
+        clocks.add(new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(6L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(6), FBUtilities.toByteArray(2L)
+            )));
+        clocks.add(new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(15L),
+            FBUtilities.toByteArray(8), FBUtilities.toByteArray(14L),
+            FBUtilities.toByteArray(4), FBUtilities.toByteArray(13L)
+            )));
+        clocks.add(new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(12L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(999L),
+            FBUtilities.toByteArray(4), FBUtilities.toByteArray(632L),
+            FBUtilities.toByteArray(8), FBUtilities.toByteArray(45L)
+            )));
+        clocks.add(new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(15L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(1234L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(655L),
+            FBUtilities.toByteArray(7), FBUtilities.toByteArray(1L)
+            )));
+
+        // 127.0.0.1:   1266L
+        // 2:           999L
+        // 3:           655L
+        // 4:           632L
+        // 1:           128L
+        // 9:           62L
+        // 8:           45L
+        // 6:           2L
+        // 7:           1L
+
+        byte[] merged = ((IncrementCounterClock)IncrementCounterClock.MIN_VALUE.getSuperset(clocks)).context();
+
+        assert 0 == FBUtilities.compareByteSubArrays(
+            FBUtilities.getLocalAddress().getAddress(),
+            0,
+            merged,
+            HEADER_LENGTH + 0*stepLength,
+            4);
+        assert 1266L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 0*stepLength + idLength);
+
+        assert    2 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 1*stepLength);
+        assert 999L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 1*stepLength + idLength);
+
+        assert    3 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 2*stepLength);
+        assert 655L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 2*stepLength + idLength);
+
+        assert    4 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 3*stepLength);
+        assert 632L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 3*stepLength + idLength);
+
+        assert    1 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 4*stepLength);
+        assert 128L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 4*stepLength + idLength);
+
+        assert   9 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 5*stepLength);
+        assert 62L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 5*stepLength + idLength);
+
+        assert   8 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 6*stepLength);
+        assert 45L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 6*stepLength + idLength);
+
+        assert   6 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 7*stepLength);
+        assert  2L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 7*stepLength + idLength);
+
+        assert   7 == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 8*stepLength);
+        assert  1L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 8*stepLength + idLength);
+    }
+
+    @Test
+    public void testCleanNodeCounts() throws UnknownHostException
+    {
+        IncrementCounterClock clock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(912L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(35L),
+            FBUtilities.toByteArray(6), FBUtilities.toByteArray(15L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(6L),
+            FBUtilities.toByteArray(7), FBUtilities.toByteArray(1L)
+            ));
+        byte[] bytes = clock.context();
+
+        assert   9 == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 3*stepLength);
+        assert  6L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 3*stepLength + idLength);
+
+        clock.cleanNodeCounts(InetAddress.getByAddress(FBUtilities.toByteArray(9)));
+        bytes = clock.context();
+
+        // node: 0.0.0.9 should be removed
+        assert HEADER_LENGTH + 4 * stepLength == bytes.length;
+
+        // verify that the other nodes are unmodified
+        assert    5 == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 0*stepLength);
+        assert 912L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 0*stepLength + idLength);
+
+        assert   3 == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 1*stepLength);
+        assert 35L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 1*stepLength + idLength);
+
+        assert   6 == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 2*stepLength);
+        assert 15L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 2*stepLength + idLength);
+
+        assert   7 == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 3*stepLength);
+        assert  1L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 3*stepLength + idLength);
+    }
+
+    @Test
+    public void testSerializeDeserialize() throws IOException, UnknownHostException
+    {
+        IncrementCounterClock clock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(912L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(35L),
+            FBUtilities.toByteArray(6), FBUtilities.toByteArray(15L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(6L),
+            FBUtilities.toByteArray(7), FBUtilities.toByteArray(1L)
+            ));
+
+        // size
+        DataOutputBuffer bufOut = new DataOutputBuffer();
+        IncrementCounterClock.SERIALIZER.serialize(clock, bufOut);
+
+        assert bufOut.getLength() == clock.size();
+
+        // equality
+        ByteArrayInputStream bufIn = new ByteArrayInputStream(bufOut.getData(), 0, bufOut.getLength());
+        IncrementCounterClock deserialized = (IncrementCounterClock)IncrementCounterClock.SERIALIZER.deserialize(new DataInputStream(bufIn));
+
+        assert 0 == FBUtilities.compareByteArrays(clock.context(), deserialized.context());
+    }
+}
Index: test/unit/org/apache/cassandra/db/SuperColumnTest.java
===================================================================
--- test/unit/org/apache/cassandra/db/SuperColumnTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/db/SuperColumnTest.java	(working copy)
@@ -22,8 +22,11 @@
 import static junit.framework.Assert.assertNotNull;
 import static junit.framework.Assert.assertNull;
 import static org.apache.cassandra.Util.getBytes;
+import static org.apache.cassandra.Util.concatByteArrays;
+import org.apache.cassandra.db.clock.IncrementCounterReconciler;
 import org.apache.cassandra.db.clock.TimestampReconciler;
 import org.apache.cassandra.db.marshal.LongType;
+import org.apache.cassandra.utils.FBUtilities;
 
 public class SuperColumnTest
 {   
@@ -34,4 +37,66 @@
     	assertNotNull(sc.getSubColumn(getBytes(1)));
     	assertNull(sc.getSubColumn(getBytes(2)));
     }
+
+    @Test
+    public void testAddColumnIncrementCounter()
+    {
+    	SuperColumn sc = new SuperColumn("sc1".getBytes(), LongType.instance, ClockType.IncrementCounter, IncrementCounterReconciler.instance);
+
+    	sc.addColumn(new Column(getBytes(1), "value".getBytes(), new IncrementCounterClock(concatByteArrays(
+            FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(7L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(5L),
+            FBUtilities.toByteArray(4), FBUtilities.toByteArray(2L)
+            ))));
+    	sc.addColumn(new Column(getBytes(1), "value".getBytes(), new IncrementCounterClock(concatByteArrays(
+            FBUtilities.toByteArray(10L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(8), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(4), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(3L)
+            ))));
+
+    	sc.addColumn(new Column(getBytes(2), "value".getBytes(), new IncrementCounterClock(concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(6L),
+            FBUtilities.toByteArray(7), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            ))));
+
+    	assertNotNull(sc.getSubColumn(getBytes(1)));
+    	assertNull(sc.getSubColumn(getBytes(3)));
+
+        // column: 1
+        assert 0 == FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)sc.getSubColumn(getBytes(1)).clock()).context(),
+            concatByteArrays(
+                FBUtilities.toByteArray(10L),
+                FBUtilities.toByteArray(0L),
+                FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(12L),
+                FBUtilities.toByteArray(8), FBUtilities.toByteArray(9L),
+                FBUtilities.toByteArray(1), FBUtilities.toByteArray(7L),
+                FBUtilities.toByteArray(2), FBUtilities.toByteArray(5L),
+                FBUtilities.toByteArray(4), FBUtilities.toByteArray(4L)
+                ));
+
+        // column: 2
+        assert 0 == FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)sc.getSubColumn(getBytes(2)).clock()).context(),
+            concatByteArrays(
+                FBUtilities.toByteArray(9L),
+                FBUtilities.toByteArray(0L),
+                FBUtilities.toByteArray(3), FBUtilities.toByteArray(6L),
+                FBUtilities.toByteArray(7), FBUtilities.toByteArray(3L),
+                FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+                ));
+
+    	assertNotNull(sc.getSubColumn(getBytes(1)));
+    	assertNotNull(sc.getSubColumn(getBytes(2)));
+    	assertNull(sc.getSubColumn(getBytes(3)));
+    }
 }
Index: test/unit/org/apache/cassandra/db/clock/IncrementCounterContextTest.java
===================================================================
--- test/unit/org/apache/cassandra/db/clock/IncrementCounterContextTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/db/clock/IncrementCounterContextTest.java	(revision 0)
@@ -0,0 +1,649 @@
+/*
+ * 
+ * Licensed to the Apache Software Foundation (ASF) under one
+ * or more contributor license agreements.  See the NOTICE file
+ * distributed with this work for additional information
+ * regarding copyright ownership.  The ASF licenses this file
+ * to you under the Apache License, Version 2.0 (the
+ * "License"); you may not use this file except in compliance
+ * with the License.  You may obtain a copy of the License at
+ * 
+ *   http://www.apache.org/licenses/LICENSE-2.0
+ * 
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ * 
+ */
+package org.apache.cassandra.db.clock;
+
+import java.net.InetAddress;
+import java.net.UnknownHostException;
+import java.util.*;
+
+import org.apache.commons.lang.ArrayUtils;
+
+import org.junit.Test;
+
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.IClock.ClockRelationship;
+import org.apache.cassandra.db.clock.IContext;
+import org.apache.cassandra.db.clock.IncrementCounterContext;
+import org.apache.cassandra.utils.FBUtilities;
+
+/**
+ * Note: these tests assume IPv4 (4 bytes) is used for id.
+ *       if IPv6 (16 bytes) is used, tests will fail (but the code will work).
+ *       however, it might be pragmatic to modify the code to just use
+ *       the IPv4 portion of the IPv6 address-space.
+ */
+public class IncrementCounterContextTest
+{
+    private static final IncrementCounterContext icc = new IncrementCounterContext();
+
+    private static final int HEADER_LENGTH;
+
+    private static final InetAddress idAddress;
+    private static final byte[] id;
+    private static final int idLength;
+    private static final int countLength;
+
+    private static final int stepLength;
+    private static final int defaultEntries;
+
+    static
+    {
+        HEADER_LENGTH  = IncrementCounterContext.HEADER_LENGTH; // 2x size of long + size of int
+
+        idAddress      = FBUtilities.getLocalAddress();
+        id             = idAddress.getAddress();
+        idLength       = 4; // size of int
+        countLength    = 8; // size of long
+        stepLength     = idLength + countLength;
+
+        defaultEntries = 10;
+    }
+
+    @Test
+    public void testCreate()
+    {
+        long start = System.currentTimeMillis();
+
+        byte[] context = icc.create();
+        assert context.length == HEADER_LENGTH;
+
+        long created = FBUtilities.byteArrayToLong(context, 0);
+        assert (start <= created);
+        assert (created <= System.currentTimeMillis());
+    }
+
+    @Test
+    public void testUpdatePresentReorder() throws UnknownHostException
+    {
+        byte[] context;
+
+        context = new byte[HEADER_LENGTH + (stepLength * defaultEntries)];
+
+        for (int i = 0; i < defaultEntries - 1; i++)
+        {
+            icc.writeElementAtStepOffset(
+                context,
+                i,
+                FBUtilities.toByteArray(i),
+                1L);
+        }
+        icc.writeElementAtStepOffset(
+            context,
+            (defaultEntries - 1),
+            id,
+            1L);
+
+        context = icc.update(context, idAddress, 10L);
+
+        assert context.length == (HEADER_LENGTH + (stepLength * defaultEntries));
+        assert 11L == FBUtilities.byteArrayToLong(context, HEADER_LENGTH + idLength);
+        for (int i = 1; i < defaultEntries; i++)
+        {
+            int offset = HEADER_LENGTH + (i * stepLength);
+            assert i-1 == FBUtilities.byteArrayToInt(context,  offset);
+        }
+    }
+
+    @Test
+    public void testUpdateNotPresent()
+    {
+        byte[] context = new byte[HEADER_LENGTH + (stepLength * 2)];
+
+        for (int i = 0; i < 2; i++)
+        {
+            icc.writeElementAtStepOffset(
+                context,
+                i,
+                FBUtilities.toByteArray(i),
+                1L);
+        }
+
+        context = icc.update(context, idAddress, 328L);
+
+        assert context.length == (HEADER_LENGTH + (stepLength * 3));
+        assert 328L == FBUtilities.byteArrayToLong(context, HEADER_LENGTH + idLength);
+        for (int i = 1; i < 3; i++)
+        {
+            int offset = HEADER_LENGTH + (i * stepLength);
+            assert i-1 == FBUtilities.byteArrayToInt(context,  offset);
+            assert  1L == FBUtilities.byteArrayToLong(context, offset + idLength);
+        }
+    }
+
+    @Test
+    public void testSwapElement()
+    {
+        byte[] context = new byte[HEADER_LENGTH + (stepLength * 3)];
+
+        for (int i = 0; i < 3; i++)
+        {
+            icc.writeElementAtStepOffset(
+                context,
+                i,
+                FBUtilities.toByteArray(i),
+                1L);
+        }
+        icc.swapElement(context, HEADER_LENGTH, HEADER_LENGTH + (2*stepLength));
+
+        assert 2 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH);
+        assert 0 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + (2*stepLength));
+
+        icc.swapElement(context, HEADER_LENGTH, HEADER_LENGTH + (1*stepLength));
+
+        assert 1 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH);
+        assert 2 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + (1*stepLength));
+    }
+
+    @Test
+    public void testPartitionElements()
+    {
+        byte[] context = new byte[HEADER_LENGTH + stepLength * 10];
+
+        icc.writeElementAtStepOffset(context, 0, FBUtilities.toByteArray(5), 1L);
+        icc.writeElementAtStepOffset(context, 1, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(context, 2, FBUtilities.toByteArray(6), 1L);
+        icc.writeElementAtStepOffset(context, 3, FBUtilities.toByteArray(7), 1L);
+        icc.writeElementAtStepOffset(context, 4, FBUtilities.toByteArray(8), 1L);
+        icc.writeElementAtStepOffset(context, 5, FBUtilities.toByteArray(9), 1L);
+        icc.writeElementAtStepOffset(context, 6, FBUtilities.toByteArray(2), 1L);
+        icc.writeElementAtStepOffset(context, 7, FBUtilities.toByteArray(4), 1L);
+        icc.writeElementAtStepOffset(context, 8, FBUtilities.toByteArray(1), 1L);
+        icc.writeElementAtStepOffset(context, 9, FBUtilities.toByteArray(3), 1L);
+
+        icc.partitionElements(
+            context,
+            0, // left
+            9, // right (inclusive)
+            2  // pivot
+            );
+
+        assert 5 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 0*stepLength);
+        assert 3 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 1*stepLength);
+        assert 3 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 2*stepLength);
+        assert 2 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 3*stepLength);
+        assert 4 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 4*stepLength);
+        assert 1 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 5*stepLength);
+        assert 6 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 6*stepLength);
+        assert 8 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 7*stepLength);
+        assert 9 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 8*stepLength);
+        assert 7 == FBUtilities.byteArrayToInt(context, HEADER_LENGTH + 9*stepLength);
+    }
+
+    @Test
+    public void testSortElementsById()
+    {
+        byte[] context = new byte[HEADER_LENGTH + (stepLength * 10)];
+
+        icc.writeElementAtStepOffset(context, 0, FBUtilities.toByteArray(5), 1L);
+        icc.writeElementAtStepOffset(context, 1, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(context, 2, FBUtilities.toByteArray(6), 1L);
+        icc.writeElementAtStepOffset(context, 3, FBUtilities.toByteArray(7), 1L);
+        icc.writeElementAtStepOffset(context, 4, FBUtilities.toByteArray(8), 1L);
+        icc.writeElementAtStepOffset(context, 5, FBUtilities.toByteArray(9), 1L);
+        icc.writeElementAtStepOffset(context, 6, FBUtilities.toByteArray(2), 1L);
+        icc.writeElementAtStepOffset(context, 7, FBUtilities.toByteArray(4), 1L);
+        icc.writeElementAtStepOffset(context, 8, FBUtilities.toByteArray(1), 1L);
+        icc.writeElementAtStepOffset(context, 9, FBUtilities.toByteArray(3), 1L);
+        
+        byte[] sorted = icc.sortElementsById(context);
+
+        assert 1 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 0*stepLength);
+        assert 2 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 1*stepLength);
+        assert 3 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 2*stepLength);
+        assert 3 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 3*stepLength);
+        assert 4 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 4*stepLength);
+        assert 5 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 5*stepLength);
+        assert 6 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 6*stepLength);
+        assert 7 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 7*stepLength);
+        assert 8 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 8*stepLength);
+        assert 9 == FBUtilities.byteArrayToInt(sorted, HEADER_LENGTH + 9*stepLength);
+    }
+
+    @Test
+    public void testCompare()
+    {
+        byte[] left;
+        byte[] right;
+
+        // equality:
+        //   left:  no local timestamp
+        //   right: no local timestamp
+        left  = Util.concatByteArrays(FBUtilities.toByteArray(1L), FBUtilities.toByteArray(0L));
+        right = Util.concatByteArrays(FBUtilities.toByteArray(1L), FBUtilities.toByteArray(0L));
+
+        assert ClockRelationship.EQUAL ==
+            icc.compare(left, right);
+
+        // equality:
+        //   left, right: local timestamps equal
+        left = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(2L)
+            );
+        right = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(2L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            );
+
+        assert ClockRelationship.EQUAL ==
+            icc.compare(left, right);
+
+        // greater than:
+        //   left:  local timestamp
+        //   right: no local timestamp
+        left = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(2L)
+            );
+        right = Util.concatByteArrays(
+            FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            );
+
+        assert ClockRelationship.GREATER_THAN ==
+            icc.compare(left, right);
+
+        // greater than:
+        //   left's local timestamp > right's local timestamp
+        left = Util.concatByteArrays(
+            FBUtilities.toByteArray(11L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(2L)
+            );
+        right = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(2L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            );
+
+        assert ClockRelationship.GREATER_THAN ==
+            icc.compare(left, right);
+
+        // less than:
+        //   left:  no local timestamp
+        //   right: local timestamp
+        left = Util.concatByteArrays(
+            FBUtilities.toByteArray(7L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(2L)
+            );
+        right = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(2L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            );
+
+        assert ClockRelationship.LESS_THAN ==
+            icc.compare(left, right);
+
+        // less than:
+        //   left's local timestamp < right's local timestamp
+        left = Util.concatByteArrays(
+            FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(2L)
+            );
+        right = Util.concatByteArrays(
+            FBUtilities.toByteArray(122L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(2L),
+            FBUtilities.toByteArray(3), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(2), FBUtilities.toByteArray(1L)
+            );
+
+        assert ClockRelationship.LESS_THAN ==
+            icc.compare(left, right);
+    }
+
+    @Test
+    public void testDiff()
+    {
+        byte[] left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        byte[] right;
+
+        // equality: equal nodes, all counts same
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+        right = ArrayUtils.clone(left);
+
+        assert ClockRelationship.EQUAL ==
+            icc.diff(left, right);
+
+        // greater than: left has superset of nodes (counts equal)
+        left = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3),  3L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6),  2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9),  1L);
+        icc.writeElementAtStepOffset(left, 3, FBUtilities.toByteArray(12), 0L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 1L);
+
+        assert ClockRelationship.GREATER_THAN ==
+            icc.diff(left, right);
+        
+        // less than: left has subset of nodes (counts equal)
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+
+        right = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3),  3L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6),  2L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9),  1L);
+        icc.writeElementAtStepOffset(right, 3, FBUtilities.toByteArray(12), 0L);
+
+        assert ClockRelationship.LESS_THAN ==
+            icc.diff(left, right);
+
+        // greater than: equal nodes, but left has higher counts
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 3L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 1L);
+
+        assert ClockRelationship.GREATER_THAN ==
+            icc.diff(left, right);
+
+        // less than: equal nodes, but right has higher counts
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 3L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 3L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 9L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 3L);
+
+        assert ClockRelationship.LESS_THAN ==
+            icc.diff(left, right);
+
+        // disjoint: right and left have disjoint node sets
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(4), 1L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 1L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 1L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(4), 1L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(2),  1L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6),  1L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(12), 1L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        // disjoint: equal nodes, but right and left have higher counts in differing nodes
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 3L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 1L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 5L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 2L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 3L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 1L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 1L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 9L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 5L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        // disjoint: left has more nodes, but lower counts
+        left = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3),  2L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6),  3L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9),  1L);
+        icc.writeElementAtStepOffset(left, 3, FBUtilities.toByteArray(12), 1L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 4L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 9L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 5L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+        
+        // disjoint: left has less nodes, but higher counts
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 5L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 3L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 2L);
+
+        right = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3),  4L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6),  3L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9),  2L);
+        icc.writeElementAtStepOffset(right, 3, FBUtilities.toByteArray(12), 1L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        // disjoint: mixed nodes and counts
+        left = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 5L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(9), 2L);
+
+        right = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3),  4L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6),  3L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9),  2L);
+        icc.writeElementAtStepOffset(right, 3, FBUtilities.toByteArray(12), 1L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+
+        left = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(left, 0, FBUtilities.toByteArray(3), 5L);
+        icc.writeElementAtStepOffset(left, 1, FBUtilities.toByteArray(6), 2L);
+        icc.writeElementAtStepOffset(left, 2, FBUtilities.toByteArray(7), 2L);
+        icc.writeElementAtStepOffset(left, 3, FBUtilities.toByteArray(9), 2L);
+
+        right = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(right, 0, FBUtilities.toByteArray(3), 4L);
+        icc.writeElementAtStepOffset(right, 1, FBUtilities.toByteArray(6), 3L);
+        icc.writeElementAtStepOffset(right, 2, FBUtilities.toByteArray(9), 2L);
+
+        assert ClockRelationship.DISJOINT ==
+            icc.diff(left, right);
+    }
+
+    @Test
+    public void testMerge()
+    {
+        // note: local counts aggregated; remote counts are reconciled (i.e. take max)
+
+        List<byte[]> contexts = new ArrayList<byte[]>();
+
+        byte[] bytes = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(bytes, 0, FBUtilities.toByteArray(1), 1L);
+        icc.writeElementAtStepOffset(bytes, 1, FBUtilities.toByteArray(2), 2L);
+        icc.writeElementAtStepOffset(bytes, 2, FBUtilities.toByteArray(4), 3L);
+        icc.writeElementAtStepOffset(
+            bytes,
+            3,
+            FBUtilities.getLocalAddress().getAddress(),
+            3L);
+        contexts.add(bytes);
+
+        bytes = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(bytes, 2, FBUtilities.toByteArray(5), 5L);
+        icc.writeElementAtStepOffset(bytes, 1, FBUtilities.toByteArray(4), 4L);
+        icc.writeElementAtStepOffset(
+            bytes,
+            0,
+            FBUtilities.getLocalAddress().getAddress(),
+            9L);
+        contexts.add(bytes);
+
+        byte[] merged = icc.merge(contexts);
+
+        // local node id's counts are aggregated
+        assert 0  == FBUtilities.compareByteSubArrays(
+            FBUtilities.getLocalAddress().getAddress(),
+            0,
+            merged, HEADER_LENGTH + 0*stepLength,
+            4);
+        assert 12L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 0*stepLength + idLength);
+
+        // remote node id counts are reconciled (i.e. take max)
+        assert 5  == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 1*stepLength);
+        assert 5L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 1*stepLength + idLength);
+
+        assert 4  == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 2*stepLength);
+        assert 4L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 2*stepLength + idLength);
+
+        assert 2  == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 3*stepLength);
+        assert 2L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 3*stepLength + idLength);
+
+        assert 1  == FBUtilities.byteArrayToInt(merged,  HEADER_LENGTH + 4*stepLength);
+        assert 1L == FBUtilities.byteArrayToLong(merged, HEADER_LENGTH + 4*stepLength + idLength);
+    }
+
+    @Test
+    public void testTotal()
+    {
+        List<byte[]> contexts = new ArrayList<byte[]>();
+
+        byte[] bytes = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(bytes, 0, FBUtilities.toByteArray(1), 1L);
+        icc.writeElementAtStepOffset(bytes, 1, FBUtilities.toByteArray(2), 2L);
+        icc.writeElementAtStepOffset(bytes, 2, FBUtilities.toByteArray(4), 3L);
+        icc.writeElementAtStepOffset(
+            bytes,
+            3,
+            FBUtilities.getLocalAddress().getAddress(),
+            3L);
+        contexts.add(bytes);
+
+        bytes = new byte[HEADER_LENGTH + (3 * stepLength)];
+        icc.writeElementAtStepOffset(bytes, 2, FBUtilities.toByteArray(5), 5L);
+        icc.writeElementAtStepOffset(bytes, 1, FBUtilities.toByteArray(4), 4L);
+        icc.writeElementAtStepOffset(
+            bytes,
+            0,
+            FBUtilities.getLocalAddress().getAddress(),
+            9L);
+        contexts.add(bytes);
+
+        byte[] merged = icc.merge(contexts);
+
+        // 127.0.0.1: 12 (3+9)
+        // 0.0.0.1:    1
+        // 0.0.0.2:    2
+        // 0.0.0.4:    4
+        // 0.0.0.5:    5
+
+        assert 24L == FBUtilities.byteArrayToLong(icc.total(merged));
+    }
+
+    @Test
+    public void testCleanNodeCounts() throws UnknownHostException
+    {
+        byte[] bytes = new byte[HEADER_LENGTH + (4 * stepLength)];
+        icc.writeElementAtStepOffset(bytes, 0, FBUtilities.toByteArray(1), 1L);
+        icc.writeElementAtStepOffset(bytes, 1, FBUtilities.toByteArray(2), 2L);
+        icc.writeElementAtStepOffset(bytes, 2, FBUtilities.toByteArray(4), 3L);
+        icc.writeElementAtStepOffset(bytes, 3, FBUtilities.toByteArray(8), 4L);
+
+        assert 4  == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 2*stepLength);
+        assert 3L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 2*stepLength + idLength);
+
+        bytes = icc.cleanNodeCounts(bytes, InetAddress.getByAddress(FBUtilities.toByteArray(4)));
+
+        // node: 0.0.0.4 should be removed
+        assert (HEADER_LENGTH + (3 * stepLength)) == bytes.length;
+
+        // other nodes should be unaffected
+        assert 1  == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 0*stepLength);
+        assert 1L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 0*stepLength + idLength);
+
+        assert 2  == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 1*stepLength);
+        assert 2L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 1*stepLength + idLength);
+
+        assert 8  == FBUtilities.byteArrayToInt(bytes,  HEADER_LENGTH + 2*stepLength);
+        assert 4L == FBUtilities.byteArrayToLong(bytes, HEADER_LENGTH + 2*stepLength + idLength);
+    }
+}
Index: test/unit/org/apache/cassandra/db/clock/IncrementCounterReconcilerTest.java
===================================================================
--- test/unit/org/apache/cassandra/db/clock/IncrementCounterReconcilerTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/db/clock/IncrementCounterReconcilerTest.java	(revision 0)
@@ -0,0 +1,403 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.db.clock;
+
+import static org.junit.Assert.*;
+
+import java.nio.ByteBuffer;
+import java.util.LinkedList;
+import java.util.List;
+
+import org.junit.Test;
+
+import org.apache.cassandra.Util;
+import org.apache.cassandra.db.Column;
+import org.apache.cassandra.db.DeletedColumn;
+import org.apache.cassandra.db.IClock;
+import org.apache.cassandra.db.IncrementCounterClock;
+import org.apache.cassandra.utils.FBUtilities;
+
+public class IncrementCounterReconcilerTest
+{   
+    private static final IncrementCounterReconciler reconciler = IncrementCounterReconciler.instance;
+    private static final IncrementCounterContext    icc        = new IncrementCounterContext();
+
+    @Test
+    public void testReconcileNormal()
+    {
+        IncrementCounterClock leftClock;
+        IncrementCounterClock rightClock;
+
+        Column left;
+        Column right;
+        Column reconciled;
+
+        List<IClock> clocks;
+
+        // normal + normal
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(10L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(27L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(32L)
+            ));
+        left = new Column(
+            "x".getBytes(),
+            icc.total(leftClock.context()),
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(7L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(9L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(32L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(6), FBUtilities.toByteArray(2L)
+            ));
+        right = new Column(
+            "x".getBytes(),
+            icc.total(rightClock.context()),
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        clocks = new LinkedList<IClock>();
+        clocks.add(rightClock);
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)leftClock.getSuperset(clocks)).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        // local:   27L+9L
+        // 1:       128L
+        // 5:       32L
+        // 6:       2L
+        // 9:       62L
+        assert FBUtilities.compareByteArrays(
+            FBUtilities.toByteArray((27L+9L)+128L+32L+2L+62L),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == false;
+    }
+
+    @Test
+    public void testReconcileMixed()
+    {
+        // note: check priority of delete vs. normal
+        //   if delete has a later timestamp, treat row as deleted
+        //   if normal has a later timestamp, ignore delete
+        IncrementCounterClock leftClock;
+        IncrementCounterClock rightClock;
+
+        Column left;
+        Column right;
+        Column reconciled;
+
+        List<IClock> clocks;
+
+        // normal + delete: normal has higher timestamp
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(44L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(32L)
+            ));
+        left = new Column(
+            "x".getBytes(),
+            "live".getBytes(),
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(1L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(0L)
+            ));
+        right = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(124).array(), // localDeleteTime secs
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)leftClock).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        assert FBUtilities.compareByteArrays(
+            "live".getBytes(),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == false;
+        
+        // normal + delete: delete has higher timestamp
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(32L)
+            ));
+        left = new Column(
+            "x".getBytes(),
+            "live".getBytes(),
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(100L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(0L)
+            ));
+        right = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(139).array(), // localDeleteTime secs
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)rightClock).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        assert FBUtilities.compareByteArrays(
+            ByteBuffer.allocate(4).putInt(139).array(),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == true;
+
+        // delete + normal: delete has higher timestamp
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(0L), FBUtilities.toByteArray(100L)
+            ));
+        left = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(139).array(), // localDeleteTime secs
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(4L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(32L)
+            ));
+        right = new Column(
+            "x".getBytes(),
+            "live".getBytes(),
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)leftClock).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        assert FBUtilities.compareByteArrays(
+            ByteBuffer.allocate(4).putInt(139).array(),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == true;
+
+        // delete + normal: normal has higher timestamp
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(1L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(0L)
+            ));
+        left = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(124).array(), // localDeleteTime secs
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(44L),
+            FBUtilities.toByteArray(0L),
+            FBUtilities.getLocalAddress().getAddress(), FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(1), FBUtilities.toByteArray(128L),
+            FBUtilities.toByteArray(9), FBUtilities.toByteArray(62L),
+            FBUtilities.toByteArray(5), FBUtilities.toByteArray(32L)
+            ));
+        right = new Column(
+            "x".getBytes(),
+            "live".getBytes(),
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)rightClock).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        assert FBUtilities.compareByteArrays(
+            "live".getBytes(),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == false;
+    }
+
+    @Test
+    public void testReconcileDeleted()
+    {
+        IncrementCounterClock leftClock;
+        IncrementCounterClock rightClock;
+
+        // note: merge clocks + take later localDeleteTime
+        Column left;
+        Column right;
+        Column reconciled;
+
+        List<IClock> clocks;
+
+        // delete + delete
+        leftClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(3L),
+            FBUtilities.toByteArray(0L)
+            ));
+        left = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(139).array(), // localDeleteTime secs
+            leftClock);
+
+        rightClock = new IncrementCounterClock(Util.concatByteArrays(
+            FBUtilities.toByteArray(6L),
+            FBUtilities.toByteArray(0L)
+            ));
+        right = new DeletedColumn(
+            "x".getBytes(),
+            ByteBuffer.allocate(4).putInt(124).array(), // localDeleteTime secs
+            rightClock);
+
+        reconciled = reconciler.reconcile(left, right);
+
+        clocks = new LinkedList<IClock>();
+        clocks.add(rightClock);
+        assert FBUtilities.compareByteArrays(
+            ((IncrementCounterClock)leftClock.getSuperset(clocks)).context(),
+            ((IncrementCounterClock)reconciled.clock()).context()
+            ) == 0;
+
+        assert FBUtilities.compareByteArrays(
+            FBUtilities.toByteArray(139),
+            reconciled.value()
+            ) == 0;
+
+        assert reconciled.isMarkedForDelete() == true;
+    }
+    
+    @Test
+    public void testReconcileDeletedOrder()
+    {
+        byte[] columnName = "col".getBytes();
+        // value: 1. timestamp 10. delete timestamp 0
+        Column c1 = new Column(
+            columnName,
+            FBUtilities.toByteArray(1L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(10L),
+                FBUtilities.toByteArray(0L)
+                )));
+        // value: 0. timestamp 7. delete timestamp 0
+        Column c2 = new DeletedColumn(
+            columnName,
+            FBUtilities.toByteArray(0L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(7L),
+                FBUtilities.toByteArray(0L)
+                )));
+        // value: 2. timestamp 3. delete timestamp 0
+        Column c3 = new Column(
+            columnName,
+            FBUtilities.toByteArray(2L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(3L),
+                FBUtilities.toByteArray(0L)
+                )));
+
+        // should return same value no matter the order of reconciliation
+        Column r1 = reconciler.reconcile(c3, c2);
+        Column r2 = reconciler.reconcile(r1, c1);
+        assertEquals(1, FBUtilities.byteArrayToLong(r2.value()));        
+
+        // a previous version of the code reconciled away the delete information in this test and the result were 3 instead of 1
+        // value: 1. timestamp 10. delete timestamp 0
+        c1 = new Column(
+            columnName,
+            FBUtilities.toByteArray(1L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(10L),
+                FBUtilities.toByteArray(0L)
+                )));
+        // value: 0. timestamp 7. delete timestamp 0
+        c2 = new DeletedColumn(
+            columnName,
+            FBUtilities.toByteArray(0L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(7L),
+                FBUtilities.toByteArray(0L)
+                )));
+        // value: 2. timestamp 3. delete timestamp 0
+        c3 = new Column(
+            columnName,
+            FBUtilities.toByteArray(2L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(3L),
+                FBUtilities.toByteArray(0L)
+                )));
+        
+        r1 = reconciler.reconcile(c1, c2);
+        r2 = reconciler.reconcile(r1, c3);
+        assertEquals(1, FBUtilities.byteArrayToLong(r2.value()));
+        
+        // check that the correct delete timestamp is pulled into the reconciled column
+        // value: 1. timestamp 10. delete timestamp 8
+        c1 = new Column(
+            columnName,
+            FBUtilities.toByteArray(1L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(10L),
+                FBUtilities.toByteArray(8L)
+                )));
+        // value: 0. timestamp 7. delete timestamp 0
+        c2 = new DeletedColumn(
+            columnName,
+            FBUtilities.toByteArray(0L),
+            new IncrementCounterClock(Util.concatByteArrays(
+                FBUtilities.toByteArray(7L),
+                FBUtilities.toByteArray(0L)
+                )));
+        
+        r1 = reconciler.reconcile(c1, c2);
+        IncrementCounterClock clock = (IncrementCounterClock) r1.clock();
+        assertEquals(8, FBUtilities.byteArrayToLong(clock.context, IncrementCounterContext.TIMESTAMP_LENGTH));
+    }
+}
Index: test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java
===================================================================
--- test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/io/sstable/SSTableWriterTest.java	(working copy)
@@ -45,6 +45,7 @@
 import org.apache.cassandra.dht.Range;
 import org.apache.cassandra.io.util.DataOutputBuffer;
 import org.apache.cassandra.service.StorageService;
+import org.apache.cassandra.io.sstable.IndexRecoveryProcessor;
 import org.apache.cassandra.thrift.IndexClause;
 import org.apache.cassandra.thrift.IndexExpression;
 import org.apache.cassandra.thrift.IndexOperator;
@@ -83,7 +84,7 @@
         new File(orig.desc.filenameFor(Component.PRIMARY_INDEX)).delete();
         new File(orig.desc.filenameFor(Component.FILTER)).delete();
         
-        SSTableReader sstr = SSTableWriter.recoverAndOpen(orig.desc);
+        SSTableReader sstr = SSTableWriter.recoverAndOpen(orig.desc, IndexRecoveryProcessor.instance());
         
         ColumnFamilyStore cfs = Table.open("Keyspace1").getColumnFamilyStore("Indexed1");
         cfs.addSSTable(sstr);
Index: test/unit/org/apache/cassandra/service/AntiEntropyServiceIncrementCounterTest.java
===================================================================
--- test/unit/org/apache/cassandra/service/AntiEntropyServiceIncrementCounterTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/service/AntiEntropyServiceIncrementCounterTest.java	(revision 0)
@@ -0,0 +1,15 @@
+package org.apache.cassandra.service;
+
+import org.apache.cassandra.db.IncrementCounterClock;
+
+public class AntiEntropyServiceIncrementCounterTest extends AntiEntropyServiceTestAbstract
+{
+
+    public void init()
+    {
+        tablename = "Keyspace5";
+        cfname = "IncrementCounter1";
+        clock = new IncrementCounterClock();
+    }
+    
+}
Index: test/unit/org/apache/cassandra/service/AntiEntropyServiceStandardTest.java
===================================================================
--- test/unit/org/apache/cassandra/service/AntiEntropyServiceStandardTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/service/AntiEntropyServiceStandardTest.java	(revision 0)
@@ -0,0 +1,15 @@
+package org.apache.cassandra.service;
+
+import org.apache.cassandra.db.TimestampClock;
+
+public class AntiEntropyServiceStandardTest extends AntiEntropyServiceTestAbstract
+{
+
+    public void init()
+    {
+        tablename = "Keyspace5";
+        cfname = "Standard1";
+        clock = new TimestampClock(0);
+    }
+    
+}
Index: test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java
===================================================================
--- test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/service/AntiEntropyServiceTest.java	(working copy)
@@ -1,258 +0,0 @@
-/*
-* Licensed to the Apache Software Foundation (ASF) under one
-* or more contributor license agreements.  See the NOTICE file
-* distributed with this work for additional information
-* regarding copyright ownership.  The ASF licenses this file
-* to you under the Apache License, Version 2.0 (the
-* "License"); you may not use this file except in compliance
-* with the License.  You may obtain a copy of the License at
-*
-*    http://www.apache.org/licenses/LICENSE-2.0
-*
-* Unless required by applicable law or agreed to in writing,
-* software distributed under the License is distributed on an
-* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-* KIND, either express or implied.  See the License for the
-* specific language governing permissions and limitations
-* under the License.
-*/
-package org.apache.cassandra.service;
-
-import java.net.InetAddress;
-import java.util.*;
-import java.util.concurrent.Callable;
-import java.util.concurrent.TimeUnit;
-import java.util.concurrent.ThreadPoolExecutor;
-
-import org.junit.After;
-import org.junit.Before;
-import org.junit.BeforeClass;
-import org.junit.Test;
-
-import org.apache.cassandra.CleanupHelper;
-import org.apache.cassandra.Util;
-import org.apache.cassandra.concurrent.Stage;
-import org.apache.cassandra.concurrent.StageManager;
-import org.apache.cassandra.config.DatabaseDescriptor;
-import org.apache.cassandra.db.*;
-import org.apache.cassandra.db.filter.QueryPath;
-import org.apache.cassandra.dht.IPartitioner;
-import org.apache.cassandra.dht.Range;
-import org.apache.cassandra.dht.Token;
-import org.apache.cassandra.io.PrecompactedRow;
-import org.apache.cassandra.io.util.DataOutputBuffer;
-import org.apache.cassandra.locator.AbstractReplicationStrategy;
-import org.apache.cassandra.locator.TokenMetadata;
-import org.apache.cassandra.utils.FBUtilities;
-import org.apache.cassandra.utils.MerkleTree;
-
-import static com.google.common.base.Charsets.UTF_8;
-import static org.apache.cassandra.service.AntiEntropyService.*;
-import static org.junit.Assert.assertEquals;
-import static org.junit.Assert.assertTrue;
-
-public class AntiEntropyServiceTest extends CleanupHelper
-{
-    // table and column family to test against
-    public static AntiEntropyService aes;
-
-    public static String tablename;
-    public static String cfname;
-    public static TreeRequest request;
-    public static ColumnFamilyStore store;
-    public static InetAddress LOCAL, REMOTE;
-
-    @BeforeClass
-    public static void prepareClass() throws Exception
-    {
-        LOCAL = FBUtilities.getLocalAddress();
-        tablename = "Keyspace5";
-        StorageService.instance.initServer();
-        // generate a fake endpoint for which we can spoof receiving/sending trees
-        REMOTE = InetAddress.getByName("127.0.0.2");
-        store = Table.open(tablename).getColumnFamilyStores().iterator().next();
-        cfname = store.columnFamily;
-    }
-
-    @Before
-    public void prepare() throws Exception
-    {
-        aes = AntiEntropyService.instance;
-        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
-        tmd.clearUnsafe();
-        tmd.updateNormalToken(StorageService.getPartitioner().getRandomToken(), LOCAL);
-        tmd.updateNormalToken(StorageService.getPartitioner().getMinimumToken(), REMOTE);
-        assert tmd.isMember(REMOTE);
-        
-        // random session id for each test
-        request = new TreeRequest(UUID.randomUUID().toString(), LOCAL, new CFPair(tablename, cfname));
-    }
-
-    @After
-    public void teardown() throws Exception
-    {
-        flushAES();
-    }
-
-    @Test
-    public void testValidatorPrepare() throws Throwable
-    {
-        Validator validator;
-
-        // write
-        List<RowMutation> rms = new LinkedList<RowMutation>();
-        RowMutation rm;
-        rm = new RowMutation(tablename, "key1".getBytes());
-        rm.add(new QueryPath(cfname, null, "Column1".getBytes()), "asdf".getBytes(), new TimestampClock(0));
-        rms.add(rm);
-        Util.writeColumnFamily(rms);
-
-        // sample
-        validator = new Validator(request);
-        validator.prepare(store);
-
-        // and confirm that the tree was split
-        assertTrue(validator.tree.size() > 1);
-    }
-    
-    @Test
-    public void testValidatorComplete() throws Throwable
-    {
-        Validator validator = new Validator(request);
-        validator.prepare(store);
-        validator.complete();
-
-        // confirm that the tree was validated
-        Token min = validator.tree.partitioner().getMinimumToken();
-        assert null != validator.tree.hash(new Range(min, min));
-
-        // wait for queued operations to be flushed
-        flushAES();
-    }
-
-    @Test
-    public void testValidatorAdd() throws Throwable
-    {
-        Validator validator = new Validator(request);
-        IPartitioner part = validator.tree.partitioner();
-        Token min = part.getMinimumToken();
-        Token mid = part.midpoint(min, min);
-        validator.prepare(store);
-
-        // add a row with the minimum token
-        validator.add(new PrecompactedRow(new DecoratedKey(min, "nonsense!".getBytes(UTF_8)),
-                                       new DataOutputBuffer()));
-
-        // and a row after it
-        validator.add(new PrecompactedRow(new DecoratedKey(mid, "inconceivable!".getBytes(UTF_8)),
-                                       new DataOutputBuffer()));
-        validator.complete();
-
-        // confirm that the tree was validated
-        assert null != validator.tree.hash(new Range(min, min));
-    }
-
-    @Test
-    public void testManualRepair() throws Throwable
-    {
-        AntiEntropyService.RepairSession sess = AntiEntropyService.instance.getRepairSession(tablename, cfname);
-        sess.start();
-        sess.blockUntilRunning();
-
-        // ensure that the session doesn't end without a response from REMOTE
-        sess.join(100);
-        assert sess.isAlive();
-
-        // deliver a fake response from REMOTE
-        AntiEntropyService.instance.completedRequest(new TreeRequest(sess.getName(), REMOTE, request.cf));
-
-        // block until the repair has completed
-        sess.join();
-    }
-
-    @Test
-    public void testGetNeighborsPlusOne() throws Throwable
-    {
-        // generate rf+1 nodes, and ensure that all nodes are returned
-        Set<InetAddress> expected = addTokens(1 + 1 + DatabaseDescriptor.getReplicationFactor(tablename));
-        expected.remove(FBUtilities.getLocalAddress());
-        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
-    }
-
-    @Test
-    public void testGetNeighborsTimesTwo() throws Throwable
-    {
-        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
-
-        // generate rf*2 nodes, and ensure that only neighbors specified by the ARS are returned
-        addTokens(1 + (2 * DatabaseDescriptor.getReplicationFactor(tablename)));
-        AbstractReplicationStrategy ars = StorageService.instance.getReplicationStrategy(tablename);
-        Set<InetAddress> expected = new HashSet<InetAddress>();
-        for (Range replicaRange : ars.getAddressRanges().get(FBUtilities.getLocalAddress()))
-        {
-            expected.addAll(ars.getRangeAddresses(tmd).get(replicaRange));
-        }
-        expected.remove(FBUtilities.getLocalAddress());
-        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
-    }
-
-    @Test
-    public void testDifferencer() throws Throwable
-    {
-        // generate a tree
-        Validator validator = new Validator(request);
-        validator.prepare(store);
-        validator.complete();
-        MerkleTree ltree = validator.tree;
-
-        // and a clone
-        validator = new Validator(request);
-        validator.prepare(store);
-        validator.complete();
-        MerkleTree rtree = validator.tree;
-
-        // change a range in one of the trees
-        Token min = StorageService.instance.getPartitioner().getMinimumToken();
-        ltree.invalidate(min);
-        MerkleTree.TreeRange changed = ltree.invalids(new Range(min, min)).next();
-        changed.hash("non-empty hash!".getBytes());
-
-        // difference the trees
-        Differencer diff = new Differencer(request, ltree, rtree);
-        diff.run();
-        
-        // ensure that the changed range was recorded
-        assertEquals("Wrong number of differing ranges", 1, diff.differences.size());
-        assertEquals("Wrong differing range", changed, diff.differences.get(0));
-    }
-
-    Set<InetAddress> addTokens(int max) throws Throwable
-    {
-        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
-        Set<InetAddress> endpoints = new HashSet<InetAddress>();
-        for (int i = 1; i < max; i++)
-        {
-            InetAddress endpoint = InetAddress.getByName("127.0.0." + i);
-            tmd.updateNormalToken(StorageService.getPartitioner().getRandomToken(), endpoint);
-            endpoints.add(endpoint);
-        }
-        return endpoints;
-    }
-
-    void flushAES() throws Exception
-    {
-        final ThreadPoolExecutor stage = StageManager.getStage(Stage.AE_SERVICE);
-        final Callable noop = new Callable<Object>()
-        {
-            public Boolean call()
-            {
-                return true;
-            }
-        };
-        
-        // send two tasks through the stage: one to follow existing tasks and a second to follow tasks created by
-        // those existing tasks: tasks won't recursively create more tasks
-        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
-        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
-    }
-}
Index: test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java
===================================================================
--- test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java	(revision 0)
+++ test/unit/org/apache/cassandra/service/AntiEntropyServiceTestAbstract.java	(revision 0)
@@ -0,0 +1,262 @@
+/*
+* Licensed to the Apache Software Foundation (ASF) under one
+* or more contributor license agreements.  See the NOTICE file
+* distributed with this work for additional information
+* regarding copyright ownership.  The ASF licenses this file
+* to you under the Apache License, Version 2.0 (the
+* "License"); you may not use this file except in compliance
+* with the License.  You may obtain a copy of the License at
+*
+*    http://www.apache.org/licenses/LICENSE-2.0
+*
+* Unless required by applicable law or agreed to in writing,
+* software distributed under the License is distributed on an
+* "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+* KIND, either express or implied.  See the License for the
+* specific language governing permissions and limitations
+* under the License.
+*/
+package org.apache.cassandra.service;
+
+import java.net.InetAddress;
+import java.util.*;
+import java.util.concurrent.Callable;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.ThreadPoolExecutor;
+
+import org.junit.After;
+import org.junit.Before;
+import org.junit.BeforeClass;
+import org.junit.Test;
+
+import org.apache.cassandra.CleanupHelper;
+import org.apache.cassandra.Util;
+import org.apache.cassandra.concurrent.Stage;
+import org.apache.cassandra.concurrent.StageManager;
+import org.apache.cassandra.config.DatabaseDescriptor;
+import org.apache.cassandra.db.*;
+import org.apache.cassandra.db.filter.QueryPath;
+import org.apache.cassandra.dht.IPartitioner;
+import org.apache.cassandra.dht.Range;
+import org.apache.cassandra.dht.Token;
+import org.apache.cassandra.io.CompactionIterator;
+import org.apache.cassandra.io.PrecompactedRow;
+import org.apache.cassandra.io.util.DataOutputBuffer;
+import org.apache.cassandra.locator.AbstractReplicationStrategy;
+import org.apache.cassandra.locator.TokenMetadata;
+import org.apache.cassandra.utils.FBUtilities;
+import org.apache.cassandra.utils.MerkleTree;
+
+import static com.google.common.base.Charsets.UTF_8;
+import static org.apache.cassandra.service.AntiEntropyService.*;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertTrue;
+
+public abstract class AntiEntropyServiceTestAbstract extends CleanupHelper
+{
+    // table and column family to test against
+    public AntiEntropyService aes;
+
+    public String tablename;
+    public String cfname;
+    public TreeRequest request;
+    public ColumnFamilyStore store;
+    public IClock clock;
+    public InetAddress LOCAL, REMOTE;
+
+    private boolean initialized;
+
+    public abstract void init();
+
+    @Before
+    public void prepare() throws Exception
+    {
+        if (!initialized)
+        {
+            LOCAL = FBUtilities.getLocalAddress();
+            init();      
+            StorageService.instance.initServer();
+            // generate a fake endpoint for which we can spoof receiving/sending trees
+            REMOTE = InetAddress.getByName("127.0.0.2");
+            store = Table.open(tablename).getColumnFamilyStores().iterator().next();
+            initialized = true;
+        }
+        aes = AntiEntropyService.instance;
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        tmd.clearUnsafe();
+        tmd.updateNormalToken(StorageService.getPartitioner().getRandomToken(), LOCAL);
+        tmd.updateNormalToken(StorageService.getPartitioner().getMinimumToken(), REMOTE);
+        assert tmd.isMember(REMOTE);
+        
+        // random session id for each test
+        request = new TreeRequest(UUID.randomUUID().toString(), LOCAL, new CFPair(tablename, cfname));
+    }
+
+    @After
+    public void teardown() throws Exception
+    {
+        flushAES();
+    }
+
+    @Test
+    public void testValidatorPrepare() throws Throwable
+    {
+        Validator validator;
+
+        // write
+        List<RowMutation> rms = new LinkedList<RowMutation>();
+        RowMutation rm;
+        rm = new RowMutation(tablename, "key1".getBytes());
+        rm.add(new QueryPath(cfname, null, "Column1".getBytes()), "asdf".getBytes(), clock);
+        rms.add(rm);
+        Util.writeColumnFamily(rms);
+
+        // sample
+        validator = new Validator(request);
+        validator.prepare(store);
+
+        // and confirm that the tree was split
+        assertTrue(validator.tree.size() > 1);
+    }
+    
+    @Test
+    public void testValidatorComplete() throws Throwable
+    {
+        Validator validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+
+        // confirm that the tree was validated
+        Token min = validator.tree.partitioner().getMinimumToken();
+        assert null != validator.tree.hash(new Range(min, min));
+
+        // wait for queued operations to be flushed
+        flushAES();
+    }
+
+    @Test
+    public void testValidatorAdd() throws Throwable
+    {
+        Validator validator = new Validator(request);
+        IPartitioner part = validator.tree.partitioner();
+        Token min = part.getMinimumToken();
+        Token mid = part.midpoint(min, min);
+        validator.prepare(store);
+
+        // add a row with the minimum token
+        validator.add(new PrecompactedRow(new DecoratedKey(min, "nonsense!".getBytes(UTF_8)),
+                                       new DataOutputBuffer()));
+
+        // and a row after it
+        validator.add(new PrecompactedRow(new DecoratedKey(mid, "inconceivable!".getBytes(UTF_8)),
+                                       new DataOutputBuffer()));
+        validator.complete();
+
+        // confirm that the tree was validated
+        assert null != validator.tree.hash(new Range(min, min));
+    }
+
+    @Test
+    public void testManualRepair() throws Throwable
+    {
+        AntiEntropyService.RepairSession sess = AntiEntropyService.instance.getRepairSession(tablename, cfname);
+        sess.start();
+        sess.blockUntilRunning();
+
+        // ensure that the session doesn't end without a response from REMOTE
+        sess.join(100);
+        assert sess.isAlive();
+
+        // deliver a fake response from REMOTE
+        AntiEntropyService.instance.completedRequest(new TreeRequest(sess.getName(), REMOTE, request.cf));
+
+        // block until the repair has completed
+        sess.join();
+    }
+
+    @Test
+    public void testGetNeighborsPlusOne() throws Throwable
+    {
+        // generate rf+1 nodes, and ensure that all nodes are returned
+        Set<InetAddress> expected = addTokens(1 + 1 + DatabaseDescriptor.getReplicationFactor(tablename));
+        expected.remove(FBUtilities.getLocalAddress());
+        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
+    }
+
+    @Test
+    public void testGetNeighborsTimesTwo() throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+
+        // generate rf*2 nodes, and ensure that only neighbors specified by the ARS are returned
+        addTokens(1 + (2 * DatabaseDescriptor.getReplicationFactor(tablename)));
+        AbstractReplicationStrategy ars = StorageService.instance.getReplicationStrategy(tablename);
+        Set<InetAddress> expected = new HashSet<InetAddress>();
+        for (Range replicaRange : ars.getAddressRanges().get(FBUtilities.getLocalAddress()))
+        {
+            expected.addAll(ars.getRangeAddresses(tmd).get(replicaRange));
+        }
+        expected.remove(FBUtilities.getLocalAddress());
+        assertEquals(expected, AntiEntropyService.getNeighbors(tablename));
+    }
+
+    @Test
+    public void testDifferencer() throws Throwable
+    {
+        // generate a tree
+        Validator validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+        MerkleTree ltree = validator.tree;
+
+        // and a clone
+        validator = new Validator(request);
+        validator.prepare(store);
+        validator.complete();
+        MerkleTree rtree = validator.tree;
+
+        // change a range in one of the trees
+        Token min = StorageService.instance.getPartitioner().getMinimumToken();
+        ltree.invalidate(min);
+        MerkleTree.TreeRange changed = ltree.invalids(new Range(min, min)).next();
+        changed.hash("non-empty hash!".getBytes());
+
+        // difference the trees
+        Differencer diff = new Differencer(request, ltree, rtree);
+        diff.run();
+        
+        // ensure that the changed range was recorded
+        assertEquals("Wrong number of differing ranges", 1, diff.differences.size());
+        assertEquals("Wrong differing range", changed, diff.differences.get(0));
+    }
+    
+    Set<InetAddress> addTokens(int max) throws Throwable
+    {
+        TokenMetadata tmd = StorageService.instance.getTokenMetadata();
+        Set<InetAddress> endpoints = new HashSet<InetAddress>();
+        for (int i = 1; i < max; i++)
+        {
+            InetAddress endpoint = InetAddress.getByName("127.0.0." + i);
+            tmd.updateNormalToken(StorageService.getPartitioner().getRandomToken(), endpoint);
+            endpoints.add(endpoint);
+        }
+        return endpoints;
+    }
+
+    void flushAES() throws Exception
+    {
+        final ThreadPoolExecutor stage = StageManager.getStage(Stage.AE_SERVICE);
+        final Callable noop = new Callable<Object>()
+        {
+            public Boolean call()
+            {
+                return true;
+            }
+        };
+        
+        // send two tasks through the stage: one to follow existing tasks and a second to follow tasks created by
+        // those existing tasks: tasks won't recursively create more tasks
+        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
+        stage.submit(noop).get(5000, TimeUnit.MILLISECONDS);
+    }
+}
Index: test/unit/org/apache/cassandra/streaming/BootstrapTest.java
===================================================================
--- test/unit/org/apache/cassandra/streaming/BootstrapTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/streaming/BootstrapTest.java	(working copy)
@@ -37,7 +37,7 @@
     public void testGetNewNames() throws IOException
     {
         Descriptor desc = Descriptor.fromFilename(new File("Keyspace1", "Standard1-500-Data.db").toString());
-        PendingFile inContext = new PendingFile(desc, "Data.db", Arrays.asList(new Pair<Long,Long>(0L, 1L)));
+        PendingFile inContext = new PendingFile(desc, "Data.db", Arrays.asList(new Pair<Long,Long>(0L, 1L)), OperationType.BOOTSTRAP);
 
         PendingFile outContext = StreamIn.getContextMapping(inContext);
         // filename and generation are expected to have changed
Index: test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java
===================================================================
--- test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/streaming/StreamingTransferTest.java	(working copy)
@@ -68,7 +68,7 @@
         List<Range> ranges = new ArrayList<Range>();
         ranges.add(new Range(p.getMinimumToken(), p.getToken("key".getBytes())));
         ranges.add(new Range(p.getToken("key2".getBytes()), p.getMinimumToken()));
-        StreamOut.transferSSTables(new StreamContext(LOCAL), tablename, Arrays.asList(sstable), ranges);
+        StreamOut.transferSSTables(new StreamContext(LOCAL), tablename, Arrays.asList(sstable), ranges, OperationType.BOOTSTRAP);
 
         // confirm that the SSTable was transferred and registered
         ColumnFamilyStore cfstore = Table.open(tablename).getColumnFamilyStore(cfname);
@@ -108,7 +108,7 @@
         List<Range> ranges = new ArrayList<Range>();
         ranges.add(new Range(p.getMinimumToken(), p.getToken("transfer1".getBytes())));
         ranges.add(new Range(p.getToken("test2".getBytes()), p.getMinimumToken()));
-        StreamOut.transferSSTables(new StreamContext(LOCAL), tablename, Arrays.asList(sstable, sstable2), ranges);
+        StreamOut.transferSSTables(new StreamContext(LOCAL), tablename, Arrays.asList(sstable, sstable2), ranges, OperationType.BOOTSTRAP);
 
         // confirm that the SSTable was transferred and registered
         ColumnFamilyStore cfstore = Table.open(tablename).getColumnFamilyStore(cfname);
Index: test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java
===================================================================
--- test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java	(revision 0)
+++ test/unit/org/apache/cassandra/thrift/ThriftValidationTest.java	(revision 0)
@@ -0,0 +1,40 @@
+package org.apache.cassandra.thrift;
+
+import org.apache.cassandra.db.IncrementCounterClock;
+import org.apache.cassandra.utils.FBUtilities;
+import org.junit.Test;
+
+public class ThriftValidationTest
+{
+
+    byte[] empty = new byte[0];
+    byte[] oneInt = FBUtilities.toByteArray(1);
+    byte[] oneLong = FBUtilities.toByteArray(1L);
+    byte[] oneNegLong = FBUtilities.toByteArray(-1L);
+    
+    @Test(expected=InvalidRequestException.class)
+    public void testValidateValueByClockEmpty() throws InvalidRequestException
+    {
+        ThriftValidation.validateValueByClock(empty, new IncrementCounterClock());
+    }
+
+    @Test(expected=InvalidRequestException.class)
+    public void testValidateValueByClockInt() throws InvalidRequestException
+    {
+        ThriftValidation.validateValueByClock(oneInt, new IncrementCounterClock());
+    }
+
+    @Test(expected=InvalidRequestException.class)
+    public void testValidateValueByClockNegLong() throws InvalidRequestException
+    {
+        ThriftValidation.validateValueByClock(oneNegLong, new IncrementCounterClock());
+    }
+
+    @Test
+    public void testValidateValueByClockLong() throws InvalidRequestException
+    {
+        ThriftValidation.validateValueByClock(oneLong, new IncrementCounterClock());
+    }
+
+    
+}
Index: test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java
===================================================================
--- test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java	(revision 996701)
+++ test/unit/org/apache/cassandra/utils/FBUtilitiesTest.java	(working copy)
@@ -20,7 +20,9 @@
 
 import static org.junit.Assert.assertArrayEquals;
 import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.fail;
 
+import java.net.UnknownHostException;
 import java.io.IOException;
 import java.nio.charset.CharacterCodingException;
 import java.util.Arrays;
@@ -28,6 +30,7 @@
 
 import org.apache.cassandra.db.IClock;
 import org.apache.cassandra.db.TimestampClock;
+import org.apache.cassandra.db.IClock.ClockRelationship;
 
 
 import org.junit.Test;
@@ -88,12 +91,95 @@
             assertEquals(i, actual);
         }
     }
+
+    @Test
+    public void testCopyIntoBytes()
+    {
+        int i = 300;
+        long l = 1000;
+        byte[] b = new byte[20];
+        FBUtilities.copyIntoBytes(b, 0, i);
+        FBUtilities.copyIntoBytes(b, 4, l);
+        assertEquals(i, FBUtilities.byteArrayToInt(b, 0));
+        assertEquals(l, FBUtilities.byteArrayToLong(b, 4));
+    }
     
     @Test
-    public void testAtomicSetMaxIClock()
+    public void testLongBytesConversions()
     {
-        AtomicReference<IClock> atomicClock = new AtomicReference<IClock>(null);
+        // positive, negative, 1 and 2 byte cases, including
+        // a few edges that would foul things up unless you're careful
+        // about masking away sign extension.
+        long[] longs = new long[]
+        {
+            -20L, -127L, -128L, 0L, 1L, 127L, 128L, 65534L, 65535L, -65534L, -65535L,
+            4294967294L, 4294967295L, -4294967294L, -4294967295L
+        };
+
+        for (long l : longs) {
+            byte[] ba = FBUtilities.toByteArray(l);
+            long actual = FBUtilities.byteArrayToLong(ba);
+            assertEquals(l, actual);
+        }
+    }
+
+    @Test
+    public void testCompareByteSubArrays()
+    {
+        byte[] bytes = new byte[16];
+
+        // handle null
+        assert FBUtilities.compareByteSubArrays(
+                null, 0, null, 0, 0) == 0;
+        assert FBUtilities.compareByteSubArrays(
+                null, 0, FBUtilities.toByteArray(524255231), 0, 4) == -1;
+        assert FBUtilities.compareByteSubArrays(
+                FBUtilities.toByteArray(524255231), 0, null, 0, 4) == 1;
+
+        // handle comparisons
+        FBUtilities.copyIntoBytes(bytes, 3, 524255231);
+        assert FBUtilities.compareByteSubArrays(
+                bytes, 3, FBUtilities.toByteArray(524255231), 0, 4) == 0;
+        assert FBUtilities.compareByteSubArrays(
+                bytes, 3, FBUtilities.toByteArray(524255232), 0, 4) == -1;
+        assert FBUtilities.compareByteSubArrays(
+                bytes, 3, FBUtilities.toByteArray(524255230), 0, 4) == 1;
+
+        // check that incorrect length throws exception
+        try
+        {
+            assert FBUtilities.compareByteSubArrays(
+                    bytes, 3, FBUtilities.toByteArray(524255231), 0, 24) == 0;
+            fail("Should raise an AssertionError.");
+        } catch (AssertionError ae)
+        {
+        }
+        try
+        {
+            assert FBUtilities.compareByteSubArrays(
+                    bytes, 3, FBUtilities.toByteArray(524255231), 0, 12) == 0;
+            fail("Should raise an AssertionError.");
+        } catch (AssertionError ae)
+        {
+        }
+    }
+    
+    @Test
+    public void testAtomicSetMax()
+    {
+        IClock clock1 = new TimestampClock(123);
+        IClock clock2 = new TimestampClock(345);
+        AtomicReference<IClock> clockRef = new AtomicReference<IClock>(clock1);
         
+        FBUtilities.atomicSetMax(clockRef, clock2);
+        assertEquals(ClockRelationship.EQUAL, clock2.compare(clockRef.get()));
+    }
+
+    @Test
+    public void testAtomicSetMaxIClock() throws UnknownHostException
+    {
+        AtomicReference<IClock> atomicClock = new AtomicReference<IClock>(null);
+
         // atomic < new
         atomicClock.set(TimestampClock.MIN_VALUE);
         FBUtilities.atomicSetMax(atomicClock, new TimestampClock(1L));
